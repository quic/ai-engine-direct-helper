import os
from typing import List, Generator, Union
from langchain_core.document_loaders import BaseLoader
from langchain_community.document_loaders import Docx2txtLoader, PyPDFLoader, PythonLoader, TextLoader, JSONLoader
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader
from chat import Chat
import sys

class PPTXLoader(BaseLoader):
    def __init__(self, file_path):
        
        self.file_path = file_path
        self.content = ""

    def extract_text_frame_info(self, text_frame):
        if text_frame and hasattr(text_frame, 'paragraphs'):
            for paragraph in text_frame.paragraphs:
                for run in paragraph.runs:
                    self.content += " " + run.text

    def extract_table_info(self, table):
        for row_idx, row in enumerate(table.rows):
            for col_idx, cell in enumerate(row.cells):
                self.extract_text_frame_info(cell.text_frame)

    def extract_shape_info(self, shape):
        if shape.shape_type == MSO_SHAPE_TYPE.TABLE:
            self.extract_table_info(shape.table)
        elif shape.has_text_frame:
            self.extract_text_frame_info(shape.text_frame)
        elif isinstance(shape, GroupShape):
            for sub_shape in shape.shapes:
                self.extract_shape_info(sub_shape)

    def extract_pptx_info(self, pptx_path):
        prs = Presentation(pptx_path)
        for slide_index, slide in enumerate(prs.slides):
            for shape in slide.shapes:
                if shape.shape_type == MSO_SHAPE_TYPE.TABLE:
                    self.extract_table_info(shape.table)
                elif shape.has_text_frame:
                    self.extract_text_frame_info(shape.text_frame)
                elif isinstance(shape, GroupShape):
                    for sub_shape in shape.shapes:
                        self.extract_shape_info(sub_shape)

    def load(self):
        self.extract_pptx_info(self.file_path)
        documents = []
        documents.append(Document(page_content=self.content, metadata={"source": self.file_path}))
        return documents

class ManualDocSummarizer:
    def __init__(self):
        """
        :param service: æœ‰ chat(prompt) æ–¹æ³•ï¼Œè¿”å› generatorï¼ˆæµå¼ tokenï¼‰
        :param llm_tokenizer: å¯é€‰ï¼Œç”¨äºä¼°ç®— token æ•°é‡ï¼ˆå¦‚ tiktokenï¼‰
        """
        self.llm_service = Chat()
        self.split_documents = []
        self.is_initialized = False

    def _estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬ token æ•°é‡"""
        return self.llm_service.textsplit(text, max_length=2048)

    def load_and_split_docs(self, file_path: str, chunk_size: int = 1024):
        """
        ä½¿ç”¨ LangChain åŠ è½½å¹¶åˆ‡åˆ†æ–‡æ¡£
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"è·¯å¾„ä¸å­˜åœ¨: {file_path}")

        documents = []

        # å®šä¹‰å„ç§åŠ è½½å™¨
        loaders = [
            (file_path, "**/*.pdf", PyPDFLoader, {"mode": "single"}),
            (file_path, "**/*.docx", Docx2txtLoader, {}),
            (file_path, "**/*.pptx", PPTXLoader, {}),
            (file_path, "**/*.txt", TextLoader, {"encoding": "utf-8"}),
            (file_path, "**/*.md", TextLoader, {"encoding": "utf-8"}),
            (file_path, "**/*.py", PythonLoader, {}),
            (file_path, "**/*.c", TextLoader, {"encoding": "utf-8"}),
            (file_path, "**/*.cpp", TextLoader, {"encoding": "utf-8"}),
            (file_path, "**/*.h", TextLoader, {"encoding": "utf-8"}),
            (file_path, "**/*.hpp", TextLoader, {"encoding": "utf-8"}),
        ]

        for path, glob, loader_cls, kwargs in loaders:
            try:
                loader = DirectoryLoader(
                    path,
                    glob=glob,
                    loader_cls=loader_cls,
                    loader_kwargs=kwargs,
                    use_multithreading=True,
                )
                docs = loader.load()
                documents.extend(docs)
            except Exception as e:
                print(f"åŠ è½½ {glob} æ—¶å‡ºé”™: {e}")

        if not documents:
            raise ValueError("æœªåŠ è½½åˆ°ä»»ä½•æ–‡æ¡£")

        self.full_text = "\n\n".join([doc.page_content.strip() for doc in documents if doc.page_content.strip()])
        self.split_documents = self.llm_service.textsplit(self.full_text, max_length=chunk_size)
        self.is_initialized = True
        return f"âœ… å·²åŠ è½½å¹¶åˆ‡åˆ†æ–‡æ¡£ï¼Œå…± {len(self.split_documents)} ä¸ªç‰‡æ®µ\n"

    def _stream_chat(self, prompt: str) -> Generator[str, None, str]:
        """
        è°ƒç”¨ service.chat å¹¶æµå¼è¿”å›ç»“æœã€‚
        æ³¨æ„ï¼šå¿…é¡»ç¡®ä¿ generator è¢«å®Œå…¨æ¶ˆè´¹ï¼Œé¿å…å¹¶å‘ã€‚
        """
        try:
            # è¿™é‡Œ chat() è¿”å›ä¸€ä¸ª generator
            token_gen = self.llm_service.chat(prompt)
            response = ""
            for token in token_gen:
                response += token
                yield token  # å®æ—¶è¾“å‡ºç»™å¤–å±‚
            # âœ… generator è¢«å®Œå…¨æ¶ˆè´¹ï¼Œè¯·æ±‚ç»“æŸ
        except Exception as e:
            error = f"[ERROR] chat è°ƒç”¨å¤±è´¥: {e}"
            yield error
            return

    def summarize_map_reduce(self, custom_prompt: str = "", max_chunk_tokens: int = 1024) -> Generator[str, None, str]:
        if not self.is_initialized:
            yield "âŒ è¯·å…ˆè°ƒç”¨ load_and_split_docs\n"
            return

        yield "ğŸš€ å¼€å§‹ Map é˜¶æ®µï¼šé€æ®µç”Ÿæˆæ‘˜è¦...\n"

        intermediate_summaries = []
        total_chunks = len(self.split_documents)
        docs = []
        for doc in self.split_documents:
            docs.append(doc["text"])

        for idx, text in enumerate(docs):
            if not text.strip():
                continue

            map_prompt = f"""
            ä½ çš„å·¥ä½œæ˜¯ç”¨ä¸­æ–‡å†™å‡ºä»¥ä¸‹æ–‡æ¡£çš„æ‘˜è¦:
            ================æ–‡æ¡£å†…å®¹å¼€å§‹================
            {text}
            ================æ–‡æ¡£å†…å®¹ç»“æŸ================
            ç¼–å†™æ‘˜è¦çš„é™„åŠ è¯´æ˜æˆ–è¦æ±‚ï¼š{custom_prompt}
            ä¸­æ–‡æ‘˜è¦:
            """
            yield f"ğŸ“Œ æ­£åœ¨å¤„ç†ç¬¬ {idx+1}/{total_chunks} æ®µ...\n"
            yield "ğŸ’¬ æ‘˜è¦ç”Ÿæˆä¸­ï¼š"

            summary_tokens = []
            try:
                for token in self._stream_chat(map_prompt):
                    summary_tokens.append(token)
                    yield token  # å®æ—¶è½¬å‘ç»™å‰ç«¯
                summary = ''.join(summary_tokens)
                yield "\nâœ… ç¬¬ %d æ®µæ‘˜è¦å®Œæˆ\n\n" % (idx+1)
            except Exception as e:
                yield f"\nâŒ ç¬¬ {idx+1} æ®µç”Ÿæˆå¤±è´¥: {e}\n"
                summary = ""

            intermediate_summaries.append(summary)

        if not intermediate_summaries:
            yield "âŒ æœªç”Ÿæˆä»»ä½•æ‘˜è¦\n"
            return
        if total_chunks > 1:
            yield "ğŸ”— å¼€å§‹ Reduce é˜¶æ®µï¼šåˆå¹¶æ‰€æœ‰æ‘˜è¦...\n"
            combined = "\n\n".join([f"[æ‘˜è¦ {i+1}]\n{s}" for i, s in enumerate(intermediate_summaries)])

            reduce_prompt = f"""
                            ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šæ–‡æ¡£æ‘˜è¦åŠ©æ‰‹ã€‚è¯·å°†ä»¥ä¸‹å¤šä¸ªæ®µè½çš„æ‘˜è¦æ•´åˆæˆä¸€ç¯‡è¿è´¯ã€ç®€æ´çš„æœ€ç»ˆæ‘˜è¦ï¼Œçªå‡ºæ•´ä½“æ ¸å¿ƒè¦ç‚¹ï¼Œé¿å…é‡å¤ã€‚

                            ç¼–å†™æ‘˜è¦çš„é™„åŠ è¯´æ˜æˆ–è¦æ±‚ï¼š{custom_prompt}\n

                            è¦åˆå¹¶çš„æ®µè½å†…å®¹ï¼š{combined}\n

                            æœ€ç»ˆä¸­æ–‡æ‘˜è¦ï¼š
                            """
            yield "ğŸ¯ æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ‘˜è¦...\n"
            # âœ… åŒæ ·ï¼šå®Œå…¨æ¶ˆè´¹ reduce é˜¶æ®µçš„ generator
            for token in self._stream_chat(reduce_prompt):
                yield token
   

    def summarize_refine(self, custom_prompt: str = "", max_chunk_tokens: int = 3500) -> Generator[str, None, str]:
        """
        æ‰‹åŠ¨å®ç° refine æ¨¡å¼ï¼šé€æ­¥ç²¾ç‚¼æ‘˜è¦
        """
        if not self.is_initialized:
            yield "âŒ è¯·å…ˆè°ƒç”¨ load_and_split_docs"
            return

        yield "ğŸ”„ å¼€å§‹ Refine æ¨¡å¼ï¼šé€æ­¥ç²¾ç‚¼æ‘˜è¦...\n"

        current_summary = ""

        for idx, doc in enumerate(self.split_documents):
            text = doc["text"]
            if not text:
                continue


            if idx == 0:
                prompt = f"""ç¼–å†™æ‘˜è¦çš„é™„åŠ è¯´æ˜æˆ–è¦æ±‚ï¼š{custom_prompt}\n
                è¯·å¯¹ä»¥ä¸‹æ–‡æœ¬è¿›è¡Œé¦–æ¬¡æ‘˜è¦ï¼Œç®€æ´æ˜äº†ï¼š
                {text}
                é¦–æ¬¡æ‘˜è¦ï¼š
                """
                yield "ğŸ“ ç”Ÿæˆé¦–æ¬¡æ‘˜è¦...\n"
            else:
                prompt = f"""
                ä½ çš„å·¥ä½œæ˜¯ç¼–å†™æœ€ç»ˆæ‘˜è¦
                æˆ‘ä»¬å·²ç»æä¾›äº†ä¸€å®šç¨‹åº¦çš„ç°æœ‰æ‘˜è¦ï¼š
                ================ç°æœ‰æ‘˜è¦å¼€å§‹================
                {current_summary}
                ================ç°æœ‰æ‘˜è¦ç»“æŸ================
                æˆ‘ä»¬æœ‰æœºä¼šå®Œå–„ç°æœ‰çš„æ‘˜è¦"
                (only if needed) ä¸‹é¢æœ‰æ›´å¤šèƒŒæ™¯ä¿¡æ¯ï¼š
                ================æ–°çš„èƒŒæ™¯ä¿¡æ¯å¼€å§‹================
                {text}\n"
                ================æ–°çš„èƒŒæ™¯ä¿¡æ¯ç»“æŸ================
                é‰´äºæ–°çš„èƒŒæ™¯ä¿¡æ¯ï¼Œå®Œå–„æ‘˜è¦ã€‚\n"
                ç¼–å†™æ‘˜è¦çš„é™„åŠ è¯´æ˜æˆ–è¦æ±‚ï¼š{custom_prompt}\nä¸­æ–‡æ‘˜è¦:
                """
                yield f"ğŸ” æ­£åœ¨èåˆç¬¬ {idx+1} æ®µä¿¡æ¯...\n"

            new_summary = ""
            for token in self._stream_chat(prompt):
                new_summary += token
            current_summary = new_summary

            yield f"âœ… å½“å‰æ‘˜è¦å·²æ›´æ–°ï¼ˆå…± {idx+1}/{len(self.split_documents)} æ®µï¼‰\n\n"

        yield "âœ… Refine æ¨¡å¼å®Œæˆï¼æœ€ç»ˆæ‘˜è¦ï¼š\n"
        yield current_summary


if __name__ == "__main__":
    docx = ManualDocSummarizer()
    docx.load_and_split_docs(file_path="dir")

    for text in docx.summarize_refine(custom_prompt="ä½ çš„å·¥ä½œæ˜¯ç”¨ä¸­æ–‡å†™å‡ºä»¥ä¸‹æ–‡æ¡£çš„æ‘˜è¦:"):
        print(text, end="")
