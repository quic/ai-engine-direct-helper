{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58c1979",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890eeaa",
   "metadata": {},
   "source": [
    "## How to run an image classification model\n",
    "\n",
    "This tutorial demonstrates how to run a classification model using QAI AppBuilder. Before running the tutorial, refer to the requirements below:\n",
    "\n",
    "- Install and setup the environment according to [Python setup](python.md). \n",
    "- Make sure the dependency libraries from Qualcomm速 AI Runtime SDK are ready. To check this, you can see whether there are files in `sample/qai_libs`. You can get more information from [User Guide](user_guide.md#environment-setup) on how to prepare these libraries manually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f964ea",
   "metadata": {},
   "source": [
    "## Install Python dependency extension:\n",
    "Run the following commands to install Python dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib==3.10.3 matplotlib-inline==0.1.7\n",
    "! pip install notebook==7.4.3 jupyter-client==8.6.3 ipykernel==6.29.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23a34e",
   "metadata": {},
   "source": [
    "## Download imagenet_classes.txt\n",
    " \n",
    "Using the following code to download the 'imagenet_classes.txt' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16288b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../samples/python\")\n",
    "import utils.install as install\n",
    "\n",
    "IMAGENET_CLASSES_URL = \"https://raw.githubusercontent.com/pytorch/hub/refs/heads/master/imagenet_classes.txt\"\n",
    "IMAGENET_CLASSES_FILE = \"imagenet_classes.txt\"\n",
    "MODEL_NAME = \"inception_v3\"\n",
    "\n",
    "workspace = os.getcwd()\n",
    "model_dir = os.path.abspath(os.path.join(workspace, \"inception_v3\", \"models\"))  # The model directory is under docs/inception_v3/models\n",
    "model_path = os.path.join(model_dir, f\"{MODEL_NAME}.bin\")\n",
    "imagenet_classes_path = os.path.join(model_dir, IMAGENET_CLASSES_FILE)\n",
    "\n",
    "if not os.path.exists(imagenet_classes_path):\n",
    "    if not install.download_url_pywget(IMAGENET_CLASSES_URL, imagenet_classes_path):\n",
    "        print(\"Error while downloading imagenet classes label text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeabd01",
   "metadata": {},
   "source": [
    "## Download model files\n",
    "\n",
    "For this tutorial, we use pretrained InceptionV3 model. We can download the QNN model from [Qualcomm AI Hub](https://aihub.qualcomm.com) directly. Open [inception_v3](https://aihub.qualcomm.com/compute/models/inception_v3) page and click the button 'Download Model', then select the below options for model downloading: <br>\n",
    "Choose runtime: Qualcomm速 AI Engine Direct. <br>\n",
    "Choose precision: Both float and w8a8 are acceptable. <br>\n",
    "Choose device: Snapdragon速 X Elite. <br>\n",
    "\n",
    "Save the downloaded model to path: docs\\inception_v3\\models\\inception_v3.dlc\n",
    "\n",
    "Refer to [DLC2BIN](../tools/convert/dlc2bin/) to convert inception_v3.dlc to inception_v3.bin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6704c",
   "metadata": {},
   "source": [
    "## Setup QNN environment\n",
    "\n",
    "After [Python setup](python.md), you should find dynamic libraries files of Qualcomm速 AI Runtime SDK under the path: 'samples/qnn_libs'.\n",
    "\n",
    "Then, you can configure the AppBuilder and load pre-trained model, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e298eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qai_appbuilder import (QNNContext, Runtime, LogLevel, ProfilingLevel, PerfProfile, QNNConfig)\n",
    "\n",
    "qnn_dir = os.path.abspath(os.path.join(workspace, \"../samples/qai_libs\"))\n",
    "\n",
    "QNNConfig.Config(qnn_lib_path=qnn_dir, runtime=Runtime.HTP, log_level=LogLevel.WARN, profiling_level=ProfilingLevel.BASIC)\n",
    "\n",
    "model = QNNContext(\"inceptionV3\", model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6744d671",
   "metadata": {},
   "source": [
    "### Configurations of initialing SDK\n",
    "\n",
    "- `qnn_lib_path=qnn_dir`: used to set the SDK library path\n",
    "- `runtime=Runtime.HTP`: used to set runtimes for model execution on Qualcomm harwdware. (Either `Runtime.HTP` or `Runtime.CPU`)\n",
    "- `log_level=LogLevel.WARN`: used to set the logging level to WARN.\n",
    "- `profiling_level=ProfilingLevel.BASIC`: used to set QNN HTP Profiling. For details, please refer to the QNN SDK document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc66802",
   "metadata": {},
   "source": [
    "## Pre-Inference process\n",
    "\n",
    "Load input image and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391ae335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "image = Image.open(os.path.join(\"../samples/python/inception_v3/input.jpg\"))\n",
    "plt.imshow(image)\n",
    "\n",
    "preprocessor = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.CenterCrop(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "img = preprocessor(image)\n",
    "img = img.unsqueeze(0).numpy() # Add batch dimension\n",
    "img = np.transpose(img, (0, 2, 3, 1)) # Convert to NHWC format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881ec47",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "To run inference on the preprocessed image, set the performance profile, execute the model, and obtain the output as follows:\n",
    "\n",
    "- **Set Performance Profile:** Configure the SDK to use the BURST profile for optimal inference speed.\n",
    "- **Run Inference:** Pass the preprocessed image to the model's `Inference` method.\n",
    "- **Reset Performance Profile:** Restore the performance profile to its previous state.\n",
    "\n",
    "The output will contain the model's predictions, which can be further processed to obtain class probabilities and top predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f3a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the performance profile to BURST\n",
    "PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)\n",
    "\n",
    "# Run inference\n",
    "output = model.Inference([[img]])[0]\n",
    "\n",
    "PerfProfile.RelPerfProfileGlobal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9ccc7",
   "metadata": {},
   "source": [
    "### Post-processing and Decoding Top-K Predictions\n",
    "\n",
    "After running inference, the model outputs raw scores (logits). To interpret these results:\n",
    "\n",
    "1. **Apply Softmax:** Convert logits to probabilities using the softmax function.\n",
    "2. **Top-K Selection:** Identify the indices of the top K probabilities (e.g., top 5).\n",
    "3. **Class Decoding:** Map these indices to human-readable class names using the ImageNet class labels.\n",
    "\n",
    "This process allows you to display the most likely predicted classes for the input image, along with their associated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f826188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 1: Samoyed (0.9999)\n",
      "Prediction 2: Arctic fox (0.0001)\n",
      "Prediction 3: white wolf (0.0000)\n",
      "Prediction 4: Eskimo dog (0.0000)\n",
      "Prediction 5: Pomeranian (0.0000)\n"
     ]
    }
   ],
   "source": [
    "output = torch.from_numpy(output)\n",
    "probabilities = torch.softmax(output, dim=0)\n",
    "top5_probabilities, top5_indices = torch.topk(probabilities, 5)\n",
    "\n",
    "# Read the ImageNet classes\n",
    "with open(imagenet_classes_path, \"r\") as f:\n",
    "    imagenet_classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Print the top 5 predictions\n",
    "for i in range(5):\n",
    "    class_id = top5_indices[i]\n",
    "    class_name = imagenet_classes[class_id]\n",
    "    probability = top5_probabilities[i].item()\n",
    "    print(f\"Prediction {i + 1}: {class_name} ({probability:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34170aab",
   "metadata": {},
   "source": [
    "## Release the resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26c5059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)\n",
    "model = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
