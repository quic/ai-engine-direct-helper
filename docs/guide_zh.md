# QAI AppBuilder ä½¿ç”¨æŒ‡å—

<div align="center">
  <h2>åŸºäº QualcommÂ® AI Runtime SDK çš„å¿«é€Ÿ AI åº”ç”¨å¼€å‘æ¡†æ¶</h2>
  <p><i>ç®€å• | æ˜“ç”¨ | é«˜æ•ˆ | å¯é </i></p>
</div>

---

## ğŸ“‘ ç›®å½•

1. [ç®€ä»‹](#1-ç®€ä»‹)
   
   - 1.1 [ä»€ä¹ˆæ˜¯ QAI AppBuilderï¼Ÿ](#11-ä»€ä¹ˆæ˜¯-qai-appbuilder)
   - 1.2 [ä¸»è¦ç‰¹æ€§](#12-ä¸»è¦ç‰¹æ€§)
   - 1.3 [ç³»ç»Ÿæ¶æ„](#13-ç³»ç»Ÿæ¶æ„)
   - 1.4 [é€‚ç”¨å¹³å°](#14-é€‚ç”¨å¹³å°)

2. [ç¯å¢ƒå‡†å¤‡](#2-ç¯å¢ƒå‡†å¤‡)
   
   - 2.1 [Windows ç¯å¢ƒé…ç½®](#21-windows-ç¯å¢ƒé…ç½®)
   - 2.2 [Linux ç¯å¢ƒé…ç½®](#22-linux-ç¯å¢ƒé…ç½®)
   - 2.3 [C++ ç¯å¢ƒé…ç½®](#23-c-ç¯å¢ƒé…ç½®)

3. [Python API è¯¦è§£](#3-python-api-è¯¦è§£)
   
   - 3.1 [æ ¸å¿ƒç±»æ¦‚è§ˆ](#31-æ ¸å¿ƒç±»æ¦‚è§ˆ)
   - 3.2 [QNNConfig - å…¨å±€é…ç½®ï¼ˆå¿…éœ€ï¼‰](#32-qnnconfig---å…¨å±€é…ç½®å¿…éœ€)
   - 3.3 [QNNContext - æ ‡å‡†æ¨¡å‹ä¸Šä¸‹æ–‡ï¼ˆæ ¸å¿ƒç±»ï¼‰](#33-qnncontext---æ ‡å‡†æ¨¡å‹ä¸Šä¸‹æ–‡æ ¸å¿ƒç±»)
   - 3.4 [ç»§æ‰¿ QNNContext çš„æœ€ä½³å®è·µ](#34-ç»§æ‰¿-qnncontext-çš„æœ€ä½³å®è·µ)
   - 3.5 [å®Œæ•´ç¤ºä¾‹ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ESRGANï¼‰](#35-å®Œæ•´ç¤ºä¾‹å›¾åƒè¶…åˆ†è¾¨ç‡real-esrgan)
   - 3.6 [å®Œæ•´ç¤ºä¾‹ï¼šå›¾åƒåˆ†ç±»ï¼ˆBEiTï¼‰](#36-å®Œæ•´ç¤ºä¾‹å›¾åƒåˆ†ç±»beit)
   - 3.7 [å®Œæ•´ç¤ºä¾‹ï¼šè¯­éŸ³è¯†åˆ«ï¼ˆWhisperï¼‰- Native æ¨¡å¼](#37-å®Œæ•´ç¤ºä¾‹è¯­éŸ³è¯†åˆ«whisper---native-æ¨¡å¼)
   - 3.8 [å®Œæ•´ç¤ºä¾‹ï¼šStable Diffusion æ–‡ç”Ÿå›¾](#38-å®Œæ•´ç¤ºä¾‹stable-diffusion-æ–‡ç”Ÿå›¾)
   - 3.9 [PerfProfile - æ€§èƒ½æ¨¡å¼ç®¡ç†](#39-perfprofile---æ€§èƒ½æ¨¡å¼ç®¡ç†)
   - 3.10 [Native æ¨¡å¼è¯¦è§£ï¼ˆé«˜æ€§èƒ½ï¼‰](#310-native-æ¨¡å¼è¯¦è§£é«˜æ€§èƒ½)

4. [C++ API è¯¦è§£](#4-c-api-è¯¦è§£)
   
   - 4.1 [LibAppBuilder ç±»](#41-libappbuilder-ç±»)
   - 4.2 [æ—¥å¿—å’Œæ€§èƒ½å‡½æ•°](#42-æ—¥å¿—å’Œæ€§èƒ½å‡½æ•°)
   - 4.3 [å®Œæ•´ C++ ç¤ºä¾‹](#43-å®Œæ•´-c-ç¤ºä¾‹)

5. [é«˜çº§åŠŸèƒ½](#5-é«˜çº§åŠŸèƒ½)
   
   - 5.1 [LoRA é€‚é…å™¨æ”¯æŒ](#51-lora-é€‚é…å™¨æ”¯æŒ)
   - 5.2 [å¤šå›¾æ¨¡å‹æ”¯æŒ](#52-å¤šå›¾æ¨¡å‹æ”¯æŒ)
   - 5.3 [æ”¯æŒçš„æ¨¡å‹æ ¼å¼](#53-æ”¯æŒçš„æ¨¡å‹æ ¼å¼)

6. [æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
   
   - 6.1 [ä½¿ç”¨ Native æ¨¡å¼ï¼ˆæ¨èï¼‰](#61-ä½¿ç”¨-native-æ¨¡å¼æ¨è)
   - 6.2 [ä½¿ç”¨ Burst æ€§èƒ½æ¨¡å¼](#62-ä½¿ç”¨-burst-æ€§èƒ½æ¨¡å¼)
   - 6.3 [æ‰¹é‡æ¨ç†ä¼˜åŒ–](#63-æ‰¹é‡æ¨ç†ä¼˜åŒ–)
   - 6.4 [ä½¿ç”¨ ARM64 Pythonï¼ˆWindowsï¼‰](#64-ä½¿ç”¨-arm64-pythonwindows)

7. [å¸¸è§é—®é¢˜](#7-å¸¸è§é—®é¢˜)
   
   - 7.1 [æ¨¡å‹åŠ è½½å¤±è´¥](#71-æ¨¡å‹åŠ è½½å¤±è´¥)
   - 7.2 [æ¨ç†ç»“æœä¸æ­£ç¡®](#72-æ¨ç†ç»“æœä¸æ­£ç¡®)
   - 7.3 [å†…å­˜æ³„æ¼](#73-å†…å­˜æ³„æ¼)
   - 7.4 [Native æ¨¡å¼æ•°æ®ç±»å‹ä¸åŒ¹é…](#74-native-æ¨¡å¼æ•°æ®ç±»å‹ä¸åŒ¹é…)
   - 7.5 [C++ é“¾æ¥é”™è¯¯](#75-c-é“¾æ¥é”™è¯¯)
   - 7.6 [æ€§èƒ½ä¸ä½³](#76-æ€§èƒ½ä¸ä½³)

8. [å‚è€ƒèµ„æº](#8-å‚è€ƒèµ„æº)
   
   - 8.1 [å®˜æ–¹æ–‡æ¡£å’Œèµ„æº](#81-å®˜æ–¹æ–‡æ¡£å’Œèµ„æº)
   - 8.2 [æ•™ç¨‹å’Œåšå®¢](#82-æ•™ç¨‹å’Œåšå®¢)
   - 8.3 [ç¤ºä¾‹ä»£ç ](#83-ç¤ºä¾‹ä»£ç )
   - 8.4 [æ¨¡å‹èµ„æº](#84-æ¨¡å‹èµ„æº)

9. [å¿«é€Ÿå¼€å§‹æŒ‡å—](#9-å¿«é€Ÿå¼€å§‹æŒ‡å—)
   
   - 9.1 [ç¬¬ä¸€ä¸ª Python ç¨‹åº](#91-ç¬¬ä¸€ä¸ª-python-ç¨‹åº)
   - 9.2 [ç¬¬ä¸€ä¸ª C++ ç¨‹åº](#92-ç¬¬ä¸€ä¸ª-c-ç¨‹åº)

10. [ç‰ˆæœ¬å†å²](#10-ç‰ˆæœ¬å†å²)

11. [è®¸å¯è¯](#11-è®¸å¯è¯)

12. [å…è´£å£°æ˜](#12-å…è´£å£°æ˜)

13. [è´¡çŒ®å’Œæ”¯æŒ](#13-è´¡çŒ®å’Œæ”¯æŒ)

---

## 1. ç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯ QAI AppBuilderï¼Ÿ

QAI AppBuilderï¼ˆQuick AI Application Builderï¼‰æ˜¯ QualcommÂ® AI Runtime SDK çš„æ‰©å±•å·¥å…·ï¼Œæ—¨åœ¨**ç®€åŒ– QNN æ¨¡å‹çš„éƒ¨ç½²æµç¨‹**ã€‚å®ƒå°†å¤æ‚çš„æ¨¡å‹æ‰§è¡Œ API å°è£…æˆä¸€ç»„ç®€åŒ–çš„æ¥å£ï¼Œä½¿å¼€å‘è€…èƒ½å¤Ÿè½»æ¾åœ°åœ¨ CPUæˆ–NPU(HTP) ä¸ŠåŠ è½½æ¨¡å‹å¹¶æ‰§è¡Œæ¨ç†ï¼Œå¤§å¹…é™ä½äº†åœ¨ Windows on Snapdragon (WoS) å’Œ Linux å¹³å°ä¸Šéƒ¨ç½² AI æ¨¡å‹çš„å¤æ‚åº¦ã€‚

### 1.2 ä¸»è¦ç‰¹æ€§

- âœ… **åŒè¯­è¨€æ”¯æŒ**ï¼šåŒæ—¶æ”¯æŒ C++ å’Œ Python
- âœ… **è·¨å¹³å°**ï¼šæ”¯æŒ Windows ï¼Œ Linuxå’ŒAndroid
- âœ… **å¤šè¿è¡Œæ—¶**ï¼šæ”¯æŒ CPU å’Œ NPU(HTP) è¿è¡Œ
- âœ… **å¤§è¯­è¨€æ¨¡å‹æ”¯æŒ**ï¼šå†…ç½® Genie æ¡†æ¶æ”¯æŒ LLM
- âœ… **å¤šæ¨¡æ€æ”¯æŒ**ï¼šæ”¯æŒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹
- âœ… **çµæ´»æ•°æ®ç±»å‹**ï¼šæ”¯æŒ Float å’Œ Native æ¨¡å¼çš„è¾“å…¥è¾“å‡º
- âœ… **å¤šå›¾æ”¯æŒ**ï¼šæ”¯æŒå¤šä¸ªè®¡ç®—å›¾
- âœ… **LoRA æ”¯æŒ**ï¼šæ”¯æŒ LoRA é€‚é…å™¨åŠ¨æ€åŠ è½½
- âœ… **å¤šæ¨¡å‹æ”¯æŒ**ï¼šå¯åŒæ—¶åŠ è½½å¤šä¸ªæ¨¡å‹
- âœ… **ä¸°å¯Œç¤ºä¾‹**ï¼šæä¾› 20+ ä¸ªå¯è¿è¡Œçš„ç¤ºä¾‹ä»£ç 

### 1.3 ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         åº”ç”¨å±‚ (Application Layer)                   â”‚
â”‚    Python App / C++ App / WebUI App                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         QAI AppBuilder API Layer                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  Python Binding  â”‚    â”‚   C++ Library    â”‚       â”‚
â”‚  â”‚  (qai_appbuilder)â”‚    â”‚ (libappbuilder)  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚       QualcommÂ® AI Runtime SDK (QNN)                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  QnnHtp.dll  â”‚  â”‚  QnnCpu.dll  â”‚  â”‚QnnSystem â”‚   â”‚
â”‚  â”‚   (NPU/HTP)  â”‚  â”‚    (CPU)     â”‚  â”‚   .dll   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          Hardware (CPU / NPU(HTP))                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.4 é€‚ç”¨å¹³å°

- **Windows on Snapdragon (WoS)**ï¼šX Elite Windows
- **Linux**ï¼šQCS8550, QCM6490 Ubuntu
- **Android**: SnapdragonÂ® 8 Eliteï¼ŒSnapdragonÂ® 8 Elite Gen 5
- **æ¶æ„æ”¯æŒ**ï¼šARM64, ARM64EC

---

## 2. ç¯å¢ƒå‡†å¤‡

### 2.1 Windows ç¯å¢ƒé…ç½®

#### æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ–è½¯ä»¶

**1. å®‰è£… Git**

```bash
# ä¸‹è½½ Git for ARM64
https://github.com/dennisameling/git/releases/download/v2.47.0.windows.2/Git-2.47.0.2-arm64.exe
```

**2. å®‰è£… Python 3.12.8**

ä½¿ç”¨ **x64 ç‰ˆæœ¬** å¯è·å¾—æ›´å¥½çš„ Python æ‰©å±•ç”Ÿæ€(å½“å‰ç›¸å¯¹æ¥è¯´ï¼Œæœ‰æ›´å¤šçš„ Python æ‰©å±•å¯åœ¨x64 Python ç¯å¢ƒä¸­è¿è¡Œï¼Œè€Œå¯¹äº ARM64 Pythonï¼Œæœ‰éƒ¨åˆ†æ‰©å±•éœ€è¦è‡ªå·±ç¼–è¯‘)ï¼š

```bash
# ä¸‹è½½ Python 3.12.8 x64
https://www.python.org/ftp/python/3.12.8/python-3.12.8-amd64.exe
```

æˆ–ä½¿ç”¨ **ARM64 ç‰ˆæœ¬** ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼š

```bash
# ä¸‹è½½ Python 3.12.8 ARM64
https://www.python.org/ftp/python/3.12.8/python-3.12.8-arm64.exe
```

âš ï¸ **é‡è¦æç¤º**ï¼š

- å®‰è£…æ—¶å¿…é¡»å‹¾é€‰ "Add python.exe to PATH"
- å¦‚æœç³»ç»Ÿä¸­æœ‰å¤šä¸ª Python ç‰ˆæœ¬ï¼Œç¡®ä¿æ–°å®‰è£…çš„ç‰ˆæœ¬åœ¨ PATH ç¯å¢ƒå˜é‡çš„é¦–ä½

éªŒè¯ Python ç‰ˆæœ¬é¡ºåºï¼š

```cmd
where python
```

**3. å®‰è£… Visual C++ Redistributable**

```bash
# ä¸‹è½½å¹¶å®‰è£…
https://aka.ms/vs/17/release/vc_redist.x64.exe
https://aka.ms/vs/17/release/vc_redist.arm64.exe
```

#### æ­¥éª¤ 2ï¼šå…‹éš† QAI AppBuilder ä»“åº“

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/quic/ai-engine-direct-helper.git --recursive

# å¦‚æœå·²å…‹éš†ï¼Œæ›´æ–°ä»£ç 
cd ai-engine-direct-helper
git pull --recurse-submodules
```

#### æ­¥éª¤ 3ï¼šå®‰è£… QAI AppBuilder Python æ‰©å±•

ä» [GitHub Release](https://github.com/quic/ai-engine-direct-helper/releases) ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„ `.whl` æ–‡ä»¶ï¼š

```bash
# å¯¹äº x64 Python
pip install qai_appbuilder-{version}-cp312-cp312-win_amd64.whl

# å¯¹äº ARM64 Python
pip install qai_appbuilder-{version}-cp312-cp312-win_arm64.whl
```

ğŸ’¡ **é‡è¦æç¤º**ï¼šä» v2.0.0 ç‰ˆæœ¬å¼€å§‹ï¼ŒQAI AppBuilder Python æ‰©å±•å·²ç»åŒ…å«äº†æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åº“ï¼ˆåŒ…æ‹¬ QualcommÂ® AI Runtime SDK è¿è¡Œæ—¶åº“ï¼‰ï¼Œæ— éœ€é¢å¤–å®‰è£… QualcommÂ® AI Runtime SDKã€‚è¿™å¤§å¤§ç®€åŒ–äº† Python å¼€å‘è€…çš„ç¯å¢ƒé…ç½®è¿‡ç¨‹ã€‚

### 2.2 Linux ç¯å¢ƒé…ç½®

#### æ­¥éª¤ 1ï¼šå®‰è£…ä¾èµ–è½¯ä»¶

**1. å®‰è£…åŸºç¡€å·¥å…·**

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y git python3 python3-pip build-essential

# éªŒè¯å®‰è£…
python3 --version
pip3 --version
```

**2. å®‰è£… Python ä¾èµ–**

```bash
# å®‰è£…å¸¸ç”¨çš„ Python åº“
pip3 install numpy pillow opencv-python
```

#### æ­¥éª¤ 2ï¼šå…‹éš† QAI AppBuilder ä»“åº“

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/quic/ai-engine-direct-helper.git --recursive

# å¦‚æœå·²å…‹éš†ï¼Œæ›´æ–°ä»£ç 
cd ai-engine-direct-helper
git pull --recurse-submodules
```

#### æ­¥éª¤ 3ï¼šå®‰è£… QAI AppBuilder Python æ‰©å±•

ä» [GitHub Release](https://github.com/quic/ai-engine-direct-helper/releases) ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„ `.whl` æ–‡ä»¶ï¼š

```bash
# å¯¹äº Linux ARM64
pip3 install qai_appbuilder-{version}-cp310-cp310-linux_aarch64.whl
```

ğŸ’¡ **é‡è¦æç¤º**ï¼šä» v2.0.0 ç‰ˆæœ¬å¼€å§‹ï¼ŒQAI AppBuilder Python æ‰©å±•å·²ç»åŒ…å«äº†æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åº“ï¼ˆåŒ…æ‹¬ QualcommÂ® AI Runtime SDK è¿è¡Œæ—¶åº“ï¼‰ï¼Œæ— éœ€é¢å¤–å®‰è£… QualcommÂ® AI Runtime SDKã€‚

#### æ­¥éª¤ 4ï¼šé…ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰

å¯¹äºæŸäº› Linux å¹³å°ï¼Œå¯èƒ½éœ€è¦è®¾ç½® `ADSP_LIBRARY_PATH` ç¯å¢ƒå˜é‡ï¼š

```bash
# æ·»åŠ åˆ° ~/.bashrc æˆ– ~/.zshrc
export ADSP_LIBRARY_PATH=/path/to/qnn/libs

# ä½¿é…ç½®ç”Ÿæ•ˆ
source ~/.bashrc
```

#### Linux ä¸ Windows çš„ä¸»è¦åŒºåˆ«

| é¡¹ç›®      | Windows                          | Linux                                  |
| ------- | -------------------------------- | -------------------------------------- |
| åº“æ–‡ä»¶æ‰©å±•å  | `.dll`                           | `.so`                                  |
| åç«¯åº“     | `QnnHtp.dll`, `QnnCpu.dll`       | `libQnnHtp.so`, `libQnnCpu.so`         |
| ç³»ç»Ÿåº“     | `QnnSystem.dll`                  | `libQnnSystem.so`                      |
| Python  | `python.exe`                     | `python3`                              |
| è·¯å¾„åˆ†éš”ç¬¦   | `\` (åæ–œæ )                        | `/` (æ­£æ–œæ )                              |
| ç¯å¢ƒå˜é‡è®¾ç½®  | ç³»ç»Ÿå±æ€§ â†’ ç¯å¢ƒå˜é‡                     | `~/.bashrc` æˆ– `~/.zshrc`               |
| ç‰¹æ®Šç¯å¢ƒå˜é‡  | æ—                                 | `ADSP_LIBRARY_PATH` (æŸäº›å¹³å°éœ€è¦)           |

### 2.3 C++ ç¯å¢ƒé…ç½®

#### æ­¥éª¤ 1ï¼šä¸‹è½½é¢„ç¼–è¯‘åº“

ä» [GitHub Release](https://github.com/quic/ai-engine-direct-helper/releases) ä¸‹è½½å¯¹åº”å¹³å°çš„é¢„ç¼–è¯‘åº“ï¼š

**Windows ARM64**ï¼š
```
QAI_AppBuilder-win_arm64-{version}-Release.zip
```

**Linux ARM64**ï¼š
```
QAI_AppBuilder-linux_aarch64-{version}-Release.tar.gz
```

è§£å‹ååŒ…å«ï¼š

- **Windows**ï¼š
  - `libappbuilder.dll` - ä¸»åº“
  - `libappbuilder.lib` - å¯¼å…¥åº“
  - `LibAppBuilder.hpp` - å¤´æ–‡ä»¶
  - `Lora.hpp` - LoRA æ”¯æŒå¤´æ–‡ä»¶

- **Linux**ï¼š
  - `libappbuilder.so` - ä¸»åº“
  - `LibAppBuilder.hpp` - å¤´æ–‡ä»¶
  - `Lora.hpp` - LoRA æ”¯æŒå¤´æ–‡ä»¶

#### æ­¥éª¤ 2ï¼šé…ç½®ç¼–è¯‘ç¯å¢ƒ

##### Windows - Visual Studio é…ç½®

**å¿…éœ€çš„é¡¹ç›®é…ç½®**ï¼š

1. **åŒ…å«ç›®å½•**
   
   - é¡¹ç›®å±æ€§ â†’ C/C++ â†’ å¸¸è§„ â†’ é™„åŠ åŒ…å«ç›®å½•
   - æ·»åŠ ï¼š`$(ProjectDir)include` æˆ–å¤´æ–‡ä»¶æ‰€åœ¨è·¯å¾„

2. **åº“ç›®å½•**
   
   - é¡¹ç›®å±æ€§ â†’ é“¾æ¥å™¨ â†’ å¸¸è§„ â†’ é™„åŠ åº“ç›®å½•
   - æ·»åŠ ï¼š`$(ProjectDir)lib` æˆ– `.lib` æ–‡ä»¶æ‰€åœ¨è·¯å¾„

3. **é“¾æ¥å™¨è¾“å…¥**
   
   - é¡¹ç›®å±æ€§ â†’ é“¾æ¥å™¨ â†’ è¾“å…¥ â†’ é™„åŠ ä¾èµ–é¡¹
   - æ·»åŠ ï¼š`libappbuilder.lib`

4. **è¿è¡Œæ—¶åº“**ï¼ˆâš ï¸ é‡è¦ï¼‰
   
   - é¡¹ç›®å±æ€§ â†’ C/C++ â†’ ä»£ç ç”Ÿæˆ â†’ è¿è¡Œæ—¶åº“
   - è®¾ç½®ä¸ºï¼š**å¤šçº¿ç¨‹ DLL (/MD)**

5. **C++ æ ‡å‡†**
   
   - é¡¹ç›®å±æ€§ â†’ C/C++ â†’ è¯­è¨€ â†’ C++ è¯­è¨€æ ‡å‡†
   - è®¾ç½®ä¸ºï¼š**ISO C++17 æ ‡å‡† (/std:c++17)** æˆ–æ›´é«˜

---

## 3. Python API è¯¦è§£

### 3.1 æ ¸å¿ƒç±»æ¦‚è§ˆ

| ç±»å               | ç”¨é€”           | æ¨èä½¿ç”¨                  | è¯´æ˜                        |
| ---------------- | ------------ | --------------------- | ------------------------- |
| `QNNContext`     | æ ‡å‡†æ¨¡å‹ä¸Šä¸‹æ–‡      | âœ… æ¨è                  | æœ€å¸¸ç”¨çš„ç±»ï¼Œé€‚ç”¨äºå¤§å¤šæ•°åœºæ™¯            |
| `QNNLoraContext` | LoRA æ¨¡å‹ä¸Šä¸‹æ–‡   | âœ… æ¨è                  | æ”¯æŒåŠ¨æ€åŠ è½½ LoRA é€‚é…å™¨           |
| `QNNContextProc` | è¿›ç¨‹éš”ç¦»æ¨¡å‹ä¸Šä¸‹æ–‡    | âœ… æ¨è                  | ç”¨äºå¤šè¿›ç¨‹åœºæ™¯             |
| `QNNShareMemory` | å…±äº«å†…å­˜ç®¡ç†       | ä¸ QNNContextProc é…åˆä½¿ç”¨ | è¿›ç¨‹é—´é«˜æ•ˆæ•°æ®ä¼ è¾“                 |
| `QNNConfig`      | å…¨å±€é…ç½®ç®¡ç†       | âœ… å¿…éœ€                  | å¿…é¡»åœ¨ä½¿ç”¨å…¶ä»– API ä¹‹å‰è°ƒç”¨          |
| `LogLevel`       | æ—¥å¿—çº§åˆ«æ§åˆ¶       | âœ… æ¨è                  | ERROR, WARN, INFO, VERBOSE, DEBUG |
| `ProfilingLevel` | æ€§èƒ½åˆ†æçº§åˆ«       | å¯é€‰                    | OFF, BASIC, DETAILED      |
| `PerfProfile`    | æ€§èƒ½æ¨¡å¼ç®¡ç†       | âœ… æ¨è                  | DEFAULT, HIGH_PERFORMANCE, BURST |
| `Runtime`        | è¿è¡Œæ—¶é€‰æ‹©        | âœ… å¿…éœ€                  | HTP æˆ– CPU          |
| `DataType`       | æ•°æ®ç±»å‹æ¨¡å¼       | âœ… æ¨è                  | FLOAT æˆ– NATIVE           |
| `LoraAdapter`    | LoRA é€‚é…å™¨     | ä¸ QNNLoraContext é…åˆä½¿ç”¨ | å®šä¹‰ LoRA é€‚é…å™¨æ–‡ä»¶è·¯å¾„           |
| `GenieContext`   | å¤§è¯­è¨€æ¨¡å‹ä¸“ç”¨ä¸Šä¸‹æ–‡   | âœ… æ¨èï¼ˆLLM åœºæ™¯ï¼‰         | ä¸“ä¸º LLM ä¼˜åŒ–çš„ä¸Šä¸‹æ–‡ç±»   |

### 3.2 QNNConfig - å…¨å±€é…ç½®ï¼ˆå¿…éœ€ï¼‰

`QNNConfig` ç”¨äºé…ç½® QNN è¿è¡Œç¯å¢ƒï¼Œ**å¿…é¡»åœ¨ä½¿ç”¨å…¶ä»– API ä¹‹å‰è°ƒç”¨**ã€‚

#### API ç­¾å

```python
class QNNConfig:
    @staticmethod
    def Config(
        qnn_lib_path: str = "None",                    # QAIRTè¿è¡Œ åº“è·¯å¾„
        runtime: str = Runtime.HTP,                    # è¿è¡Œæ—¶
        log_level: int = LogLevel.ERROR,               # æ—¥å¿—çº§åˆ«
        profiling_level: int = ProfilingLevel.OFF,     # æ€§èƒ½åˆ†æçº§åˆ«
        log_path: str = "None"                         # æ—¥å¿—æ–‡ä»¶è·¯å¾„
    ) -> None
```

#### å‚æ•°è¯¦è§£

| å‚æ•°                | ç±»å‹  | é»˜è®¤å€¼                | è¯´æ˜                                                                        |
| ----------------- | --- | ------------------ | ------------------------------------------------------------------------- |
| `qnn_lib_path`    | str | "None"             | QNN åº“æ–‡ä»¶ç›®å½•è·¯å¾„ï¼ˆåŒ…å« QnnHtp.dll ç­‰ï¼Œ**ä»QAI AppBuilder v2.0.0å¼€å§‹ï¼Œä¸éœ€è¦è®¾ç½®æ­¤å‚æ•°ï¼Œé»˜è®¤ç½®ç©ºå³å¯**ï¼‰ |
| `runtime`         | str | Runtime.HTP        | `Runtime.HTP` (NPU) æˆ– `Runtime.CPU`                                       |
| `log_level`       | int | LogLevel.ERROR     | ERROR(1), WARN(2), INFO(3), VERBOSE(4), DEBUG(5)                          |
| `profiling_level` | int | ProfilingLevel.OFF | OFF(0), BASIC(1), DETAILED(2)                                             |
| `log_path`        | str | "None"             | æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼Œ"None" è¡¨ç¤ºè¾“å‡ºåˆ°æ§åˆ¶å°                                                    |

#### ä½¿ç”¨ç¤ºä¾‹

```python
from qai_appbuilder import QNNConfig, Runtime, LogLevel, ProfilingLevel

QNNConfig.Config(
    runtime=Runtime.HTP,
    log_level=LogLevel.WARN,
    profiling_level=ProfilingLevel.BASIC
)
```

### 3.3 QNNContext - æ ‡å‡†æ¨¡å‹ä¸Šä¸‹æ–‡ï¼ˆæ ¸å¿ƒç±»ï¼‰

`QNNContext` æ˜¯æœ€å¸¸ç”¨çš„ç±»ï¼Œç”¨äºåŠ è½½æ¨¡å‹ã€æ‰§è¡Œæ¨ç†å’Œç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸã€‚

#### æ„é€ å‡½æ•°

```python
class QNNContext:
    def __init__(
        self,
        model_name: str = "None",                      # æ¨¡å‹åç§°ï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
        model_path: str = "None",                      # æ¨¡å‹æ–‡ä»¶è·¯å¾„
        backend_lib_path: str = "None",                # åç«¯åº“è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        system_lib_path: str = "None",                 # ç³»ç»Ÿåº“è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        is_async: bool = False,                        # æ˜¯å¦å¼‚æ­¥æ‰§è¡Œ
        input_data_type: str = DataType.FLOAT,         # è¾“å…¥æ•°æ®ç±»å‹
        output_data_type: str = DataType.FLOAT         # è¾“å‡ºæ•°æ®ç±»å‹
    ) -> None
```

#### å‚æ•°è¯¦è§£

| å‚æ•°                 | ç±»å‹   | é»˜è®¤å€¼            | è¯´æ˜                                                                   |
| ------------------ | ---- | -------------- | -------------------------------------------------------------------- |
| `model_name`       | str  | "None"         | æ¨¡å‹å”¯ä¸€æ ‡è¯†ç¬¦ï¼Œç”¨äºåŒºåˆ†ä¸åŒæ¨¡å‹                                                     |
| `model_path`       | str  | "None"         | æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ `.bin` å’Œ `.dlc` æ ¼å¼ï¼‰                                        |
| `backend_lib_path` | str  | "None"         | QnnHtp.dll æˆ– QnnCpu.dll è·¯å¾„ï¼ˆå¯é€‰ï¼Œ**ä»QAI AppBuilder v2.0.0å¼€å§‹ï¼Œä¸éœ€è¦è®¾ç½®æ­¤å‚æ•°**ï¼‰ |
| `system_lib_path`  | str  | "None"         | QnnSystem.dll è·¯å¾„ï¼ˆå¯é€‰ï¼Œ**ä»QAI AppBuilder v2.0.0å¼€å§‹ï¼Œä¸éœ€è¦è®¾ç½®æ­¤å‚æ•°**ï¼‰           |
| `is_async`         | bool | False          | æ˜¯å¦å¯ç”¨å¼‚æ­¥æ¨ç†                                                             |
| `input_data_type`  | str  | DataType.FLOAT | `DataType.FLOAT` æˆ– `DataType.NATIVE`                                 |
| `output_data_type` | str  | DataType.FLOAT | `DataType.FLOAT` æˆ– `DataType.NATIVE`                                 |

ğŸ’¡ **æç¤º**ï¼šä»QAI AppBuilder **v2.0.0** å¼€å§‹ï¼Œä¸éœ€è¦è®¾ç½®å‚æ•°ï¼š`backend_lib_path` å’Œ `system_lib_path` ã€‚

#### æ ¸å¿ƒæ–¹æ³•

##### Inference - æ‰§è¡Œæ¨ç†

```python
def Inference(
    self,
    input: List[np.ndarray],                           # è¾“å…¥æ•°æ®åˆ—è¡¨
    perf_profile: str = PerfProfile.DEFAULT,           # æ€§èƒ½æ¨¡å¼
    graphIndex: int = 0                                # å›¾ç´¢å¼•
) -> List[np.ndarray]                                  # è¿”å›è¾“å‡ºåˆ—è¡¨
```

**å‚æ•°è¯´æ˜**ï¼š

- `input`ï¼šè¾“å…¥æ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª NumPy æ•°ç»„
- `perf_profile`ï¼šæ€§èƒ½æ¨¡å¼(ä¸æ¨èä½¿ç”¨æ­¤å‚æ•°ã€‚)
  - `PerfProfile.DEFAULT`ï¼šé»˜è®¤æ¨¡å¼ï¼ˆä¸æ”¹å˜æ€§èƒ½é…ç½®ï¼‰
  - `PerfProfile.HIGH_PERFORMANCE`ï¼šé«˜æ€§èƒ½æ¨¡å¼
  - `PerfProfile.BURST`ï¼šçªå‘æ¨¡å¼ï¼ˆæœ€é«˜æ€§èƒ½ï¼‰
- `graphIndex`ï¼šå›¾ç´¢å¼•ï¼ˆç”¨äºå¤šå›¾æ¨¡å‹ï¼Œé»˜è®¤ä¸º 0ï¼‰

**è¿”å›å€¼**ï¼š

- è¾“å‡ºæ•°æ®åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ª NumPy æ•°ç»„

ğŸ’¡ **æç¤º**ï¼šä¸æ¨èä½¿ç”¨perf_profileå‚æ•°ï¼Œå»ºè®®é€šè¿‡é…å¯¹ä½¿ç”¨PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST) ã€PerfProfile.RelPerfProfileGlobal()æ¥å®ç°è®¾ç½® NPU ä¸ºé«˜æ€§èƒ½æ¨¡å¼ã€‚

##### æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢æ–¹æ³•

```python
# è·å–è¾“å…¥å½¢çŠ¶ï¼ˆä¾‹å¦‚ï¼š[[1, 3, 224, 224]]ï¼‰
def getInputShapes(self) -> List[List[int]]

# è·å–è¾“å‡ºå½¢çŠ¶ï¼ˆä¾‹å¦‚ï¼š[[1, 1000]]ï¼‰
def getOutputShapes(self) -> List[List[int]]

# è·å–è¾“å…¥æ•°æ®ç±»å‹ï¼ˆä¾‹å¦‚ï¼š['float32'] æˆ– ['float16']ï¼‰
def getInputDataType(self) -> List[str]

# è·å–è¾“å‡ºæ•°æ®ç±»å‹ï¼ˆä¾‹å¦‚ï¼š['float32'] æˆ– ['float16']ï¼‰
def getOutputDataType(self) -> List[str]

# è·å–å›¾åç§°
def getGraphName(self) -> str

# è·å–è¾“å…¥å¼ é‡åç§°ï¼ˆä¾‹å¦‚ï¼š['input']ï¼‰
def getInputName(self) -> List[str]

# è·å–è¾“å‡ºå¼ é‡åç§°ï¼ˆä¾‹å¦‚ï¼š['output']ï¼‰
def getOutputName(self) -> List[str]
```

### 3.4 ç»§æ‰¿ QNNContext çš„æœ€ä½³å®è·µ

ç¤ºä¾‹ä»£ç ï¼Œ**ç»§æ‰¿ `QNNContext` ç±»**æ¥å°è£…ç‰¹å®šæ¨¡å‹çš„é€»è¾‘ã€‚

```python
from qai_appbuilder import QNNContext, QNNConfig, Runtime, LogLevel, PerfProfile
import numpy as np
import os

# é…ç½®ç¯å¢ƒ
QNNConfig.Config(
    runtime=Runtime.HTP,
    log_level=LogLevel.WARN
)

# ============================================
# æ–¹å¼ 1ï¼šç®€å•ç»§æ‰¿ï¼ˆæœ€å¸¸ç”¨ï¼‰
# ============================================
class RealESRGan(QNNContext):
    """Real-ESRGAN å›¾åƒè¶…åˆ†è¾¨ç‡æ¨¡å‹"""

    def Inference(self, input_data):
        """é‡å†™ Inference æ–¹æ³•ä»¥ç®€åŒ–è°ƒç”¨"""
        input_datas = [input_data]
        output_data = super().Inference(input_datas)[0]
        return output_data

# ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç±»
model_path = "models/real_esrgan_x4plus.bin"
realesrgan = RealESRGan("realesrgan", model_path)

# æ‰§è¡Œæ¨ç†
PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)
output = realesrgan.Inference(input_data)
PerfProfile.RelPerfProfileGlobal()

del realesrgan

# ============================================
# æ–¹å¼ 2ï¼šå¤šè¾“å…¥æ¨¡å‹
# ============================================
class Unet(QNNContext):
    """UNet å»å™ªæ¨¡å‹ - å¤šè¾“å…¥"""

    def Inference(self, input_data_1, input_data_2, input_data_3):
        """æ¥å—å¤šä¸ªè¾“å…¥"""
        # é‡å¡‘ä¸ºä¸€ç»´æ•°ç»„
        input_data_1 = input_data_1.reshape(input_data_1.size)
        # input_data_2 å·²ç»æ˜¯ä¸€ç»´ï¼Œä¸éœ€è¦é‡å¡‘
        input_data_3 = input_data_3.reshape(input_data_3.size)

        input_datas = [input_data_1, input_data_2, input_data_3]
        output_data = super().Inference(input_datas)[0]

        # é‡å¡‘è¾“å‡º
        output_data = output_data.reshape(1, 64, 64, 4)
        return output_data

# ä½¿ç”¨
unet = Unet("unet", "models/unet.bin")
output = unet.Inference(latent, timestep, text_embedding)
del unet

# ============================================
# æ–¹å¼ 3ï¼šå¤šè¾“å‡ºæ¨¡å‹
# ============================================
class Encoder(QNNContext):
    """Whisper Encoder - å¤šè¾“å‡º"""

    def Inference(self, input_data):
        """è¿”å›å¤šä¸ªè¾“å‡º"""
        input_datas = [input_data]
        output_data = super().Inference(input_datas)

        # é‡å¡‘æ¯ä¸ªè¾“å‡º
        k_cache_cross = output_data[0].reshape(6, 8, 64, 1500)
        v_cache_cross = output_data[1].reshape(6, 8, 1500, 64)

        return k_cache_cross, v_cache_cross

# ä½¿ç”¨
encoder = Encoder("whisper_encoder", "models/encoder.bin")
k_cache, v_cache = encoder.Inference(mel_input)
del encoder
```

### 3.5 å®Œæ•´ç¤ºä¾‹ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ESRGANï¼‰

å‚è€ƒä»£ç ï¼š`samples/python/real_esrgan_x4plus/real_esrgan_x4plus.py`

```python
from qai_appbuilder import (
    QNNContext, QNNConfig, Runtime, LogLevel, 
    ProfilingLevel, PerfProfile
)
import numpy as np
from PIL import Image
from pathlib import Path
import os

# ============================================
# 1. é…ç½® QNN ç¯å¢ƒ
# ============================================
execution_ws = Path(os.getcwd())
qnn_dir = execution_ws / "qai_libs"

# å¤„ç†ä¸åŒçš„å·¥ä½œç›®å½•æƒ…å†µ
if "python" not in str(execution_ws):
    execution_ws = execution_ws / "python"

MODEL_NAME = "real_esrgan_x4plus"
if MODEL_NAME not in str(execution_ws):
    execution_ws = execution_ws / MODEL_NAME

model_dir = execution_ws / "models"

QNNConfig.Config(
    qnn_lib_path=str(qnn_dir),  # æ­¤å‚æ•°ä» v2.0.0 å¼€å§‹å¯ä»¥ä¸è¿›è¡Œè®¾ç½®ï¼Œç•™ç©ºå³å¯ã€‚
    runtime=Runtime.HTP,
    log_level=LogLevel.WARN,
    profiling_level=ProfilingLevel.BASIC
)

# ============================================
# 2. å®šä¹‰æ¨¡å‹ç±»ï¼ˆç»§æ‰¿è‡ª QNNContextï¼‰
# ============================================
class RealESRGan(QNNContext):
    """Real-ESRGAN å›¾åƒè¶…åˆ†è¾¨ç‡æ¨¡å‹"""

    def Inference(self, input_data):
        """é‡å†™ Inference æ–¹æ³•ä»¥ç®€åŒ–è°ƒç”¨"""
        input_datas = [input_data]
        output_data = super().Inference(input_datas)[0]
        return output_data

# ============================================
# 3. åˆå§‹åŒ–æ¨¡å‹
# ============================================
IMAGE_SIZE = 512
model_path = model_dir / f"{MODEL_NAME}.bin"

# åˆ›å»ºæ¨¡å‹å®ä¾‹
realesrgan = RealESRGan("realesrgan", str(model_path))

# æŸ¥è¯¢æ¨¡å‹ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
print(f"æ¨¡å‹åç§°: {realesrgan.getGraphName()}")
print(f"è¾“å…¥å½¢çŠ¶: {realesrgan.getInputShapes()}")
print(f"è¾“å‡ºå½¢çŠ¶: {realesrgan.getOutputShapes()}")
print(f"è¾“å…¥æ•°æ®ç±»å‹: {realesrgan.getInputDataType()}")
print(f"è¾“å‡ºæ•°æ®ç±»å‹: {realesrgan.getOutputDataType()}")

# ============================================
# 4. å›¾åƒé¢„å¤„ç†è¾…åŠ©å‡½æ•°
# ============================================
def pil_resize_pad(image, target_size):
    """è°ƒæ•´å›¾åƒå¤§å°å¹¶å¡«å……åˆ°ç›®æ ‡å°ºå¯¸"""
    orig_width, orig_height = image.size
    target_width, target_height = target_size

    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
    scale = min(target_width / orig_width, target_height / orig_height)
    new_width = int(orig_width * scale)
    new_height = int(orig_height * scale)

    # è°ƒæ•´å¤§å°
    image = image.resize((new_width, new_height), Image.LANCZOS)

    # åˆ›å»ºæ–°å›¾åƒå¹¶å¡«å……
    new_image = Image.new('RGB', target_size, (0, 0, 0))
    paste_x = (target_width - new_width) // 2
    paste_y = (target_height - new_height) // 2
    new_image.paste(image, (paste_x, paste_y))

    padding = (paste_x, paste_y)
    return new_image, scale, padding

def pil_undo_resize_pad(image, original_size, scale, padding):
    """ç§»é™¤å¡«å……å¹¶æ¢å¤åˆ°åŸå§‹å°ºå¯¸"""
    # è£å‰ªå¡«å……
    width, height = image.size
    left = padding[0] * 4
    top = padding[1] * 4
    right = width - padding[0] * 4
    bottom = height - padding[1] * 4
    image = image.crop((left, top, right, bottom))

    # è°ƒæ•´åˆ°åŸå§‹å°ºå¯¸
    image = image.resize(original_size, Image.LANCZOS)
    return image

# ============================================
# 5. æ‰§è¡Œæ¨ç†
# ============================================
input_image_path = execution_ws / "input.jpg"
output_image_path = execution_ws / "output.png"

# è¯»å–å’Œé¢„å¤„ç†å›¾åƒ
orig_image = Image.open(input_image_path)
image, scale, padding = pil_resize_pad(orig_image, (IMAGE_SIZE, IMAGE_SIZE))

image = np.array(image)
image = (np.clip(image, 0, 255) / 255.0).astype(np.float32)  # å½’ä¸€åŒ–

# è®¾ç½® HTP ä¸ºçªå‘æ¨¡å¼ä»¥è·å¾—æœ€ä½³æ€§èƒ½
PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)

# æ‰§è¡Œæ¨ç†
output_image = realesrgan.Inference(image)

# é‡ç½® HTP æ€§èƒ½æ¨¡å¼
PerfProfile.RelPerfProfileGlobal()

# ============================================
# 6. åå¤„ç†
# ============================================
# é‡å¡‘è¾“å‡ºå½¢çŠ¶
output_image = output_image.reshape(IMAGE_SIZE * 4, IMAGE_SIZE * 4, 3)

# åå½’ä¸€åŒ–
output_image = np.clip(output_image, 0.0, 1.0)
output_image = (output_image * 255).astype(np.uint8)

# è½¬æ¢ä¸º PIL å›¾åƒ
output_image = Image.fromarray(output_image)

# ç§»é™¤å¡«å……å¹¶æ¢å¤åŸå§‹å°ºå¯¸
image_size = (orig_image.size[0] * 4, orig_image.size[1] * 4)
image_padding = (padding[0] * 4, padding[1] * 4)
output_image = pil_undo_resize_pad(output_image, image_size, scale, image_padding)

# ä¿å­˜ç»“æœ
output_image.save(output_image_path)
print(f"è¶…åˆ†è¾¨ç‡å›¾åƒå·²ä¿å­˜åˆ°: {output_image_path}")

# ============================================
# 7. æ¸…ç†èµ„æº
# ============================================
del realesrgan
```

### 3.6 å®Œæ•´ç¤ºä¾‹ï¼šå›¾åƒåˆ†ç±»ï¼ˆBEiTï¼‰

å‚è€ƒä»£ç ï¼š`samples/python/beit/beit.py`

```python
from qai_appbuilder import (
    QNNContext, QNNConfig, Runtime, LogLevel, 
    ProfilingLevel, PerfProfile
)
import numpy as np
import torch
import torchvision.transforms as transforms
from PIL import Image
from pathlib import Path
import os

# ============================================
# 1. é…ç½®ç¯å¢ƒ
# ============================================
execution_ws = Path(os.getcwd())
qnn_dir = execution_ws / "qai_libs"

if "python" not in str(execution_ws):
    execution_ws = execution_ws / "python"

MODEL_NAME = "beit"
if MODEL_NAME not in str(execution_ws):
    execution_ws = execution_ws / MODEL_NAME

model_dir = execution_ws / "models"

QNNConfig.Config(
    qnn_lib_path=str(qnn_dir),  # æ­¤å‚æ•°ä» v2.0.0 å¼€å§‹å¯ä»¥ä¸è¿›è¡Œè®¾ç½®ï¼Œç•™ç©ºå³å¯ã€‚
    runtime=Runtime.HTP,
    log_level=LogLevel.WARN,
    profiling_level=ProfilingLevel.BASIC
)

# ============================================
# 2. å®šä¹‰ BEiT æ¨¡å‹ç±»
# ============================================
class Beit(QNNContext):
    """BEiT å›¾åƒåˆ†ç±»æ¨¡å‹"""

    def Inference(self, input_data):
        input_datas = [input_data]
        output_data = super().Inference(input_datas)[0]
        return output_data

# ============================================
# 3. åˆå§‹åŒ–æ¨¡å‹
# ============================================
IMAGE_SIZE = 224
model_path = model_dir / f"{MODEL_NAME}.bin"

beit = Beit("beit", str(model_path))

# ============================================
# 4. å›¾åƒé¢„å¤„ç†
# ============================================
def preprocess_PIL_image(image: Image) -> torch.Tensor:
    """é¢„å¤„ç† PIL å›¾åƒ"""
    preprocess = transforms.Compose([
        transforms.Resize(IMAGE_SIZE),
        transforms.CenterCrop(IMAGE_SIZE),
        transforms.ToTensor(),
    ])

    img = preprocess(image)
    img = img.unsqueeze(0)
    return img

# ============================================
# 5. æ‰§è¡Œæ¨ç†
# ============================================
input_image_path = execution_ws / "input.jpg"

# è¯»å–å’Œé¢„å¤„ç†å›¾åƒ
image = Image.open(input_image_path)
image = preprocess_PIL_image(image).numpy()
image = np.transpose(image, (0, 2, 3, 1))  # NCHW -> NHWC

# è®¾ç½®çªå‘æ¨¡å¼
PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)

# æ‰§è¡Œæ¨ç†
output_data = beit.Inference(image)

# é‡ç½®æ€§èƒ½æ¨¡å¼
PerfProfile.RelPerfProfileGlobal()

# ============================================
# 6. åå¤„ç†
# ============================================
# è½¬æ¢ä¸º torch tensor å¹¶åº”ç”¨ softmax
output = torch.from_numpy(output_data).squeeze(0)
probabilities = torch.softmax(output, dim=0)

# è·å– Top-5 é¢„æµ‹
top5_prob, top5_catid = torch.topk(probabilities, 5)

print("\nTop 5 é¢„æµ‹ç»“æœ:")
for i in range(5):
    print(f"{i+1}. ç±»åˆ« {top5_catid[i]}: {top5_prob[i].item():.6f}")

# ============================================
# 7. æ¸…ç†èµ„æº
# ============================================
del beit
```

### 3.7 å®Œæ•´ç¤ºä¾‹ï¼šè¯­éŸ³è¯†åˆ«ï¼ˆWhisperï¼‰- Native æ¨¡å¼

å‚è€ƒä»£ç ï¼š`samples/python/whisper_base_en/whisper_base_en.py`

```python
from qai_appbuilder import (
    QNNContext, QNNConfig, Runtime, LogLevel, 
    ProfilingLevel, PerfProfile, DataType
)
import numpy as np
import torch
import audio2numpy as a2n
import samplerate
import whisper
from pathlib import Path
import os

# ============================================
# 1. é…ç½®ç¯å¢ƒ
# ============================================
execution_ws = Path(os.getcwd())
qnn_dir = execution_ws / "qai_libs"

if "python" not in str(execution_ws):
    execution_ws = execution_ws / "python"

MODEL_NAME = "whisper_base_en"
if MODEL_NAME not in str(execution_ws):
    execution_ws = execution_ws / MODEL_NAME

model_dir = execution_ws / "models"

QNNConfig.Config(
    qnn_lib_path=str(qnn_dir),  # æ­¤å‚æ•°ä» v2.0.0 å¼€å§‹å¯ä»¥ä¸è¿›è¡Œè®¾ç½®ï¼Œç•™ç©ºå³å¯ã€‚
    runtime=Runtime.HTP,
    log_level=LogLevel.WARN,
    profiling_level=ProfilingLevel.BASIC
)

# ============================================
# 2. å®šä¹‰ Encoder å’Œ Decoder ç±»ï¼ˆä½¿ç”¨ Native æ¨¡å¼ï¼‰
# ============================================
class Encoder(QNNContext):
    """Whisper Encoder - ä½¿ç”¨ Native æ¨¡å¼"""

    def Inference(self, input_data):
        input_datas = [input_data]
        output_data = super().Inference(input_datas)

        # é‡å¡‘è¾“å‡º
        k_cache_cross = output_data[0].reshape(6, 8, 64, 1500)
        v_cache_cross = output_data[1].reshape(6, 8, 1500, 64)

        return k_cache_cross, v_cache_cross

class Decoder(QNNContext):
    """Whisper Decoder - ä½¿ç”¨ Native æ¨¡å¼"""

    def Inference(self, x, index, k_cache_cross, v_cache_cross, k_cache_self, v_cache_self):
        input_datas = [x, index, k_cache_cross, v_cache_cross, k_cache_self, v_cache_self]
        output_data = super().Inference(input_datas)

        # é‡å¡‘è¾“å‡º
        logits = output_data[0].reshape(1, 1, 51864)
        k_cache = output_data[1].reshape(6, 8, 64, 224)
        v_cache = output_data[2].reshape(6, 8, 224, 64)

        return logits, k_cache, v_cache

# ============================================
# 3. åˆå§‹åŒ–æ¨¡å‹ï¼ˆNative æ¨¡å¼ï¼‰
# ============================================
encoder_model_path = model_dir / "whisper_base_en-whisperencoder.bin"
decoder_model_path = model_dir / "whisper_base_en-whisperdecoder.bin"

# ä½¿ç”¨ Native æ¨¡å¼åˆå§‹åŒ–
encoder = Encoder(
    "whisper_encoder",
    str(encoder_model_path),
    input_data_type=DataType.NATIVE,
    output_data_type=DataType.NATIVE
)

decoder = Decoder(
    "whisper_decoder",
    str(decoder_model_path),
    input_data_type=DataType.NATIVE,
    output_data_type=DataType.NATIVE
)

# æŸ¥çœ‹æ¨¡å‹çš„åŸç”Ÿæ•°æ®ç±»å‹
print("\nEncoder æ¨¡å‹ä¿¡æ¯:")
print(f"  è¾“å…¥æ•°æ®ç±»å‹: {encoder.getInputDataType()}")
print(f"  è¾“å‡ºæ•°æ®ç±»å‹: {encoder.getOutputDataType()}")

print("\nDecoder æ¨¡å‹ä¿¡æ¯:")
print(f"  è¾“å…¥æ•°æ®ç±»å‹: {decoder.getInputDataType()}")
print(f"  è¾“å‡ºæ•°æ®ç±»å‹: {decoder.getOutputDataType()}")

# ============================================
# 4. Whisper å¸¸é‡å®šä¹‰
# ============================================
TOKEN_SOT = 50257  # Start of transcript
TOKEN_EOT = 50256  # End of transcript
SAMPLE_RATE = 16000
CHUNK_LENGTH = 30  # seconds
MAX_AUDIO_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE

# ============================================
# 5. éŸ³é¢‘é¢„å¤„ç†å‡½æ•°
# ============================================
def log_mel_spectrogram(audio_np: np.ndarray) -> np.ndarray:
    """è®¡ç®— Mel é¢‘è°±å›¾ï¼ˆè¿”å› float16ï¼‰"""
    audio = torch.from_numpy(audio_np)

    # å¡«å……éŸ³é¢‘åˆ°å›ºå®šé•¿åº¦
    padding = MAX_AUDIO_SAMPLES - len(audio)
    if padding > 0:
        audio = torch.nn.functional.pad(audio, (0, padding))

    # è®¡ç®— STFT
    n_fft = 400
    hop_length = 160
    window = torch.hann_window(n_fft)
    stft = torch.stft(audio, n_fft, hop_length, window=window, return_complex=True)
    magnitudes = stft[..., :-1].abs() ** 2

    # åº”ç”¨ Mel æ»¤æ³¢å™¨ï¼ˆéœ€è¦é¢„å…ˆåŠ è½½ï¼‰
    # mel_filter = np.load("mel_filters.npz")["mel_80"]
    # mel_spec = torch.from_numpy(mel_filter) @ magnitudes

    # è®¡ç®— log mel spectrogram
    log_spec = torch.clamp(magnitudes, min=1e-10).log10()
    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)
    log_spec = (log_spec + 4.0) / 4.0

    # è¿”å› float16ï¼ˆNative æ¨¡å¼ï¼‰
    return log_spec.unsqueeze(0).to(dtype=torch.float16).cpu().numpy()

# ============================================
# 6. æ‰§è¡Œæ¨ç†
# ============================================
audio_path = execution_ws / "jfk.wav"

# è¯»å–éŸ³é¢‘æ–‡ä»¶
audio, audio_sample_rate = a2n.audio_from_file(str(audio_path))

# é‡é‡‡æ ·åˆ° 16kHz
if audio_sample_rate != SAMPLE_RATE:
    audio = samplerate.resample(audio, SAMPLE_RATE / audio_sample_rate)

# è®¡ç®— Mel é¢‘è°±å›¾ï¼ˆè¿”å› float16ï¼‰
mel_input = log_mel_spectrogram(audio)
print(f"mel_input: dtype={mel_input.dtype}, shape={mel_input.shape}")

# è®¾ç½®çªå‘æ¨¡å¼
PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)

# Encoder æ¨ç†
print("æ‰§è¡Œ Encoder æ¨ç†...")
k_cache_cross, v_cache_cross = encoder.Inference(mel_input)

print(f"k_cache_cross: shape={k_cache_cross.shape}, dtype={k_cache_cross.dtype}")
print(f"v_cache_cross: shape={v_cache_cross.shape}, dtype={v_cache_cross.dtype}")

# ============================================
# 7. Decoder æ¨ç†ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰
# ============================================
# åˆå§‹åŒ– Decoder è¾“å…¥
x = np.array([[TOKEN_SOT]], dtype=np.int32)
index = np.array([[0]], dtype=np.int32)
k_cache_self = np.zeros((6, 8, 64, 224), dtype=np.float16)
v_cache_self = np.zeros((6, 8, 224, 64), dtype=np.float16)

decoded_tokens = [TOKEN_SOT]
max_tokens = 100

print("\næ‰§è¡Œ Decoder æ¨ç†ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰...")
for i in range(max_tokens):
    index = np.array([[i]], dtype=np.int32)

    # Decoder æ¨ç†
    logits, k_cache_self, v_cache_self = decoder.Inference(
        x, index, k_cache_cross, v_cache_cross, k_cache_self, v_cache_self
    )

    # è·å–ä¸‹ä¸€ä¸ª token
    next_token = np.argmax(logits[0, -1])
    decoded_tokens.append(int(next_token))

    # æ£€æŸ¥æ˜¯å¦ç»“æŸ
    if next_token == TOKEN_EOT:
        break

    # æ›´æ–°è¾“å…¥
    x = np.array([[next_token]], dtype=np.int32)

    if (i + 1) % 10 == 0:
        print(f"  å·²ç”Ÿæˆ {i + 1} ä¸ª tokens...")

# é‡ç½®æ€§èƒ½æ¨¡å¼
PerfProfile.RelPerfProfileGlobal()

print(f"\nç”Ÿæˆå®Œæˆï¼Œå…± {len(decoded_tokens)} ä¸ª tokens")

# ============================================
# 8. è§£ç  tokens ä¸ºæ–‡æœ¬
# ============================================
tokenizer = whisper.decoding.get_tokenizer(
    multilingual=False, language="en", task="transcribe"
)
text = tokenizer.decode(decoded_tokens[1:])  # ç§»é™¤ TOKEN_SOT
print(f"è½¬å½•ç»“æœ: {text.strip()}")

# ============================================
# 9. æ¸…ç†èµ„æº
# ============================================
del encoder
del decoder
```

### 3.8 å®Œæ•´ç¤ºä¾‹ï¼šStable Diffusion æ–‡ç”Ÿå›¾

å‚è€ƒä»£ç ï¼š`samples/python/stable_diffusion_v1_5/stable_diffusion_v1_5.py`

```python
from qai_appbuilder import (
    QNNContext, QNNConfig, Runtime, LogLevel, 
    ProfilingLevel, PerfProfile
)
import numpy as np
from PIL import Image
from pathlib import Path
import torch
from transformers import CLIPTokenizer
from diffusers import DPMSolverMultistepScheduler
import os

# ============================================
# 1. é…ç½®ç¯å¢ƒ
# ============================================
execution_ws = Path(os.getcwd())
qnn_dir = execution_ws / "qai_libs"

if "python" not in str(execution_ws):
    execution_ws = execution_ws / "python"

MODEL_NAME = "stable_diffusion_v1_5"
if MODEL_NAME not in str(execution_ws):
    execution_ws = execution_ws / MODEL_NAME

model_dir = execution_ws / "models"

QNNConfig.Config(
    qnn_lib_path=str(qnn_dir),  # æ­¤å‚æ•°ä» v2.0.0 å¼€å§‹å¯ä»¥ä¸è¿›è¡Œè®¾ç½®ï¼Œç•™ç©ºå³å¯ã€‚
    runtime=Runtime.HTP,
    log_level=LogLevel.ERROR,
    profiling_level=ProfilingLevel.BASIC
)

# ============================================
# 2. å®šä¹‰æ¨¡å‹ç±»
# ============================================
class TextEncoder(QNNContext):
    """æ–‡æœ¬ç¼–ç å™¨"""

    def Inference(self, input_data):
        input_datas = [input_data]
        output_data = super().Inference(input_datas)[0]
        # è¾“å‡ºå½¢çŠ¶åº”è¯¥æ˜¯ (1, 77, 768)
        output_data = output_data.reshape((1, 77, 768))
        return output_data

class Unet(QNNContext):
    """UNet å»å™ªæ¨¡å‹"""

    def Inference(self, input_data_1, input_data_2, input_data_3):
        # é‡å¡‘ä¸ºä¸€ç»´æ•°ç»„
        input_data_1 = input_data_1.reshape(input_data_1.size)
        # input_data_2 å·²ç»æ˜¯ä¸€ç»´ï¼Œä¸éœ€è¦é‡å¡‘
        input_data_3 = input_data_3.reshape(input_data_3.size)

        input_datas = [input_data_1, input_data_2, input_data_3]
        output_data = super().Inference(input_datas)[0]

        # é‡å¡‘è¾“å‡ºä¸º (1, 64, 64, 4)
        output_data = output_data.reshape(1, 64, 64, 4)
        return output_data

class VaeDecoder(QNNContext):
    """VAE è§£ç å™¨"""

    def Inference(self, input_data):
        input_data = input_data.reshape(input_data.size)
        input_datas = [input_data]
        output_data = super().Inference(input_datas)[0]
        return output_data

# ============================================
# 3. åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹
# ============================================
text_encoder = TextEncoder(
    "text_encoder",
    str(model_dir / "text_encoder.bin")
)

unet = Unet(
    "model_unet",
    str(model_dir / "unet.bin")
)

vae_decoder = VaeDecoder(
    "vae_decoder",
    str(model_dir / "vae_decoder.bin")
)

# ============================================
# 4. åˆå§‹åŒ– Tokenizer å’Œ Scheduler
# ============================================
# åˆå§‹åŒ– CLIP Tokenizer
tokenizer_dir = model_dir / "tokenizer"
tokenizer = CLIPTokenizer.from_pretrained(
    "openai/clip-vit-large-patch14",
    cache_dir=str(tokenizer_dir)
)

# åˆå§‹åŒ– Scheduler
scheduler = DPMSolverMultistepScheduler(
    num_train_timesteps=1000,
    beta_start=0.00085,
    beta_end=0.012,
    beta_schedule="scaled_linear"
)

# ============================================
# 5. è®¾ç½®ç”Ÿæˆå‚æ•°
# ============================================
user_prompt = "spectacular view of northern lights from Alaska"
uncond_prompt = "lowres, text, error, cropped, worst quality"
user_seed = np.int64(42)
user_step = 20
user_text_guidance = 7.5

# ============================================
# 6. Tokenize æç¤ºè¯
# ============================================
def run_tokenizer(prompt, max_length=77):
    """Tokenize æ–‡æœ¬"""
    text_input = tokenizer(
        prompt,
        padding="max_length",
        max_length=max_length,
        truncation=True
    )
    text_input = np.array(text_input.input_ids, dtype=np.float32)
    return text_input

cond_tokens = run_tokenizer(user_prompt)
uncond_tokens = run_tokenizer(uncond_prompt)

# ============================================
# 7. æ‰§è¡Œå®Œæ•´çš„ç”Ÿæˆæµç¨‹
# ============================================
# è®¾ç½®çªå‘æ¨¡å¼
PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)

# è®¾ç½® scheduler æ—¶é—´æ­¥
scheduler.set_timesteps(user_step)

# ç¼–ç æ–‡æœ¬
print("ç¼–ç æ–‡æœ¬æç¤º...")
uncond_text_embedding = text_encoder.Inference(uncond_tokens)
user_text_embedding = text_encoder.Inference(cond_tokens)

# åˆå§‹åŒ–éšæœº latent
random_init_latent = torch.randn(
    (1, 4, 64, 64),
    generator=torch.manual_seed(user_seed)
).numpy()
latent_in = random_init_latent.transpose(0, 2, 3, 1)  # NCHW -> NHWC

# å»å™ªå¾ªç¯
print(f"å¼€å§‹å»å™ªï¼ˆ{user_step} æ­¥ï¼‰...")
for step in range(user_step):
    print(f'  æ­¥éª¤ {step + 1}/{user_step}')

    # è·å–å½“å‰æ—¶é—´æ­¥
    time_step = np.int32(scheduler.timesteps.numpy()[step])

    # UNet æ¨ç†ï¼ˆæ— æ¡ä»¶ï¼‰
    unconditional_noise_pred = unet.Inference(
        latent_in, time_step, uncond_text_embedding
    )

    # UNet æ¨ç†ï¼ˆæœ‰æ¡ä»¶ï¼‰
    conditional_noise_pred = unet.Inference(
        latent_in, time_step, user_text_embedding
    )

    # åˆå¹¶å™ªå£°é¢„æµ‹
    noise_pred_uncond = np.transpose(unconditional_noise_pred, (0, 3, 1, 2))
    noise_pred_text = np.transpose(conditional_noise_pred, (0, 3, 1, 2))
    latent_in_nchw = np.transpose(latent_in, (0, 3, 1, 2))

    noise_pred_uncond = torch.from_numpy(noise_pred_uncond)
    noise_pred_text = torch.from_numpy(noise_pred_text)
    latent_in_torch = torch.from_numpy(latent_in_nchw)

    # åº”ç”¨ guidance
    noise_pred = noise_pred_uncond + user_text_guidance * (noise_pred_text - noise_pred_uncond)

    # Scheduler æ­¥éª¤
    latent_out = scheduler.step(noise_pred, time_step, latent_in_torch).prev_sample.numpy()

    # è½¬æ¢å› NHWC
    latent_in = np.transpose(latent_out, (0, 2, 3, 1))

# VAE è§£ç 
print("è§£ç ä¸ºå›¾åƒ...")
output_image = vae_decoder.Inference(latent_in)

# é‡ç½®æ€§èƒ½æ¨¡å¼
PerfProfile.RelPerfProfileGlobal()

# ============================================
# 8. åå¤„ç†
# ============================================
image_size = 512
output_image = np.clip(output_image * 255.0, 0.0, 255.0).astype(np.uint8)
output_image = output_image.reshape(image_size, image_size, 3)
output_image = Image.fromarray(output_image, mode="RGB")

# ä¿å­˜å›¾åƒ
output_path = execution_ws / "generated_image.png"
output_image.save(output_path)
print(f"å›¾åƒå·²ä¿å­˜åˆ°: {output_path}")

# ============================================
# 9. æ¸…ç†èµ„æº
# ============================================
del text_encoder
del unet
del vae_decoder
```

### 3.9 PerfProfile - æ€§èƒ½æ¨¡å¼ç®¡ç†

`PerfProfile` ç”¨äºæ§åˆ¶ HTP (NPU) çš„æ€§èƒ½æ¨¡å¼ã€‚

#### æ€§èƒ½æ¨¡å¼å¯¹æ¯”

| æ¨¡å¼                 | è¯´æ˜    | åŠŸè€—  | æ€§èƒ½  | é€‚ç”¨åœºæ™¯       |
| ------------------ | ----- | --- | --- | ---------- |
| `DEFAULT`          | é»˜è®¤æ¨¡å¼  | ä½   | ä¸­   | ä¸æ”¹å˜æ€§èƒ½é…ç½®    |
| `HIGH_PERFORMANCE` | é«˜æ€§èƒ½æ¨¡å¼ | ä¸­   | é«˜   | æŒç»­é«˜è´Ÿè½½æ¨ç†    |
| `BURST`            | çªå‘æ¨¡å¼  | é«˜   | æœ€é«˜  | çŸ­æ—¶é—´å†…éœ€è¦æœ€é«˜æ€§èƒ½ |

#### ä½¿ç”¨æ–¹æ³•

```python
from qai_appbuilder import PerfProfile, QNNContext

model = QNNContext(...)

# ============================================
# æ–¹æ³• 1ï¼šå…¨å±€è®¾ç½®ï¼ˆæ¨èç”¨äºæ‰¹é‡æ¨ç†ï¼‰
# ============================================
# è®¾ç½®ä¸ºçªå‘æ¨¡å¼
PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)

# æ‰§è¡Œå¤šæ¬¡æ¨ç†ï¼ˆéƒ½ä½¿ç”¨çªå‘æ¨¡å¼ï¼‰
for i in range(100):
    # æ³¨æ„ï¼šperf_profile å‚æ•°è®¾ç½®ä¸º DEFAULT ä»¥ä½¿ç”¨å…¨å±€é…ç½®
    output = model.Inference([input_data], perf_profile=PerfProfile.DEFAULT)

# é‡Šæ”¾æ€§èƒ½æ¨¡å¼
PerfProfile.RelPerfProfileGlobal()

# ============================================
# æ–¹æ³• 2ï¼šå•æ¬¡æ¨ç†è®¾ç½®
# ============================================
# æ¯æ¬¡æ¨ç†æ—¶æŒ‡å®šæ€§èƒ½æ¨¡å¼
output = model.Inference([input_data], perf_profile=PerfProfile.BURST)
```

âš ï¸ **é‡è¦æç¤º**ï¼š

- ä½¿ç”¨ `SetPerfProfileGlobal()` åï¼Œ`Inference()` çš„ `perf_profile` å‚æ•°åº”è®¾ç½®ä¸º `PerfProfile.DEFAULT`
- å¦‚æœåœ¨ `Inference()` ä¸­æŒ‡å®šå…¶ä»–æ€§èƒ½æ¨¡å¼ï¼Œä¼šè¦†ç›–å…¨å±€è®¾ç½®

### 3.10 Native æ¨¡å¼è¯¦è§£ï¼ˆé«˜æ€§èƒ½ï¼‰

Native æ¨¡å¼ç›´æ¥ä½¿ç”¨æ¨¡å‹çš„åŸç”Ÿæ•°æ®ç±»å‹ï¼Œé¿å…æ•°æ®è½¬æ¢ï¼Œ**æ˜¾è‘—æå‡æ€§èƒ½**ã€‚

#### æ”¯æŒçš„æ•°æ®ç±»å‹

- `int8` / `uint8`
- `int16` / `uint16`
- `int32` / `uint32`
- `float16` (fp16) - æœ€å¸¸ç”¨
- `float32` (fp32)
- `bool`

#### Native æ¨¡å¼ä½¿ç”¨æ­¥éª¤

```python
from qai_appbuilder import QNNContext, DataType
import numpy as np

# 1. åˆ›å»ºæ¨¡å‹æ—¶æŒ‡å®š native æ¨¡å¼
model = QNNContext(
    model_name="my_model",
    model_path="models/my_model.bin",
    input_data_type=DataType.NATIVE,
    output_data_type=DataType.NATIVE
)

# 2. æŸ¥è¯¢æ¨¡å‹æ‰€éœ€çš„æ•°æ®ç±»å‹
input_dtypes = model.getInputDataType()
output_dtypes = model.getOutputDataType()
print(f"Input data types: {input_dtypes}")   # ä¾‹å¦‚ï¼š['float16']
print(f"Output data types: {output_dtypes}") # ä¾‹å¦‚ï¼š['float16']

# 3. åˆ›å»ºæ•°æ®ç±»å‹æ˜ å°„
dtype_map = {
    'float16': np.float16,
    'fp16': np.float16,
    'float32': np.float32,
    'fp32': np.float32,
    'float': np.float32,
    'int8': np.int8,
    'uint8': np.uint8,
    'int16': np.int16,
    'uint16': np.uint16,
    'int32': np.int32,
    'uint32': np.uint32,
    'bool': np.bool_
}

# 4. æ ¹æ®æ¨¡å‹è¦æ±‚å‡†å¤‡æ•°æ®
input_dtype_str = input_dtypes[0].lower()
input_dtype = dtype_map.get(input_dtype_str, np.float32)

input_shapes = model.getInputShapes()
input_data = np.random.randn(*input_shapes[0]).astype(input_dtype)

# 5. æ‰§è¡Œæ¨ç†
outputs = model.Inference([input_data])

# 6. è¾“å‡ºä¹Ÿæ˜¯åŸç”Ÿç±»å‹
print(f"Output dtype: {outputs[0].dtype}")  # ä¾‹å¦‚ï¼šfloat16
```

---

## 4. C++ API è¯¦è§£

### 4.1 LibAppBuilder ç±»

`LibAppBuilder` æ˜¯ C++ API çš„æ ¸å¿ƒç±»ã€‚

#### å¤´æ–‡ä»¶å¼•ç”¨

```cpp
#include "LibAppBuilder.hpp"
```

#### ä¸»è¦æ–¹æ³•

##### ModelInitialize - åˆå§‹åŒ–æ¨¡å‹

```cpp
bool ModelInitialize(
    const std::string& model_name,                     // æ¨¡å‹åç§°
    const std::string& model_path,                     // æ¨¡å‹æ–‡ä»¶è·¯å¾„
    const std::string& backend_lib_path,               // åç«¯åº“è·¯å¾„
    const std::string& system_lib_path,                // ç³»ç»Ÿåº“è·¯å¾„
    bool async = false,                                // æ˜¯å¦å¼‚æ­¥
    const std::string& input_data_type = "float",      // è¾“å…¥æ•°æ®ç±»å‹
    const std::string& output_data_type = "float"      // è¾“å‡ºæ•°æ®ç±»å‹
);
```

**å‚æ•°è¯´æ˜**ï¼š

- `model_name`ï¼šæ¨¡å‹å”¯ä¸€æ ‡è¯†ç¬¦ï¼ˆå¦‚ "mobilenet_v2"ï¼‰
- `model_path`ï¼šæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ `.bin` å’Œ `.dlc`ï¼‰
- `backend_lib_path`ï¼šQnnHtp.dll æˆ– QnnCpu.dll çš„å®Œæ•´è·¯å¾„
- `system_lib_path`ï¼šQnnSystem.dll çš„å®Œæ•´è·¯å¾„
- `async`ï¼šæ˜¯å¦å¯ç”¨å¼‚æ­¥æ¨ç†
- `input_data_type`ï¼š`"float"` æˆ– `"native"`
- `output_data_type`ï¼š`"float"` æˆ– `"native"`

**è¿”å›å€¼**ï¼š

- `true`ï¼šåˆå§‹åŒ–æˆåŠŸ
- `false`ï¼šåˆå§‹åŒ–å¤±è´¥

âš ï¸ **é‡è¦æç¤º**ï¼š

- è°ƒç”¨ C++ APIï¼Œåœ¨åˆå§‹åŒ–æ—¶è¦ç¡®ä¿æ­£ç¡®å‡†å¤‡ QAIRT SDK è¿è¡Œæ—¶åº“ï¼Œå¹¶é€šè¿‡ backend_lib_path åŠ system_lib_path å‚æ•°æ­£ç¡®è®¾ç½®ç›¸åº”åº“æ–‡ä»¶è·¯å¾„ã€‚

##### ModelInference - æ‰§è¡Œæ¨ç†

```cpp
bool ModelInference(
    std::string model_name,                            // æ¨¡å‹åç§°
    std::vector<uint8_t*>& inputBuffers,               // è¾“å…¥ç¼“å†²åŒº
    std::vector<uint8_t*>& outputBuffers,              // è¾“å‡ºç¼“å†²åŒºï¼ˆå‡½æ•°åˆ†é…ï¼‰
    std::vector<size_t>& outputSize,                   // è¾“å‡ºå¤§å°
    std::string& perfProfile,                          // æ€§èƒ½æ¨¡å¼
    size_t graphIndex = 0,                             // å›¾ç´¢å¼•
);
```

**å‚æ•°è¯´æ˜**ï¼š

- `model_name`ï¼šæ¨¡å‹åç§°ï¼ˆä¸ ModelInitialize ä¸­çš„ä¸€è‡´ï¼‰
- `inputBuffers`ï¼šè¾“å…¥æ•°æ®ç¼“å†²åŒºåˆ—è¡¨ï¼ˆ`uint8_t*` æŒ‡é’ˆï¼‰
- `outputBuffers`ï¼šè¾“å‡ºæ•°æ®ç¼“å†²åŒºåˆ—è¡¨ï¼ˆ**å‡½æ•°ä¼šè‡ªåŠ¨åˆ†é…å†…å­˜**ï¼‰
- `outputSize`ï¼šè¾“å‡ºæ•°æ®å¤§å°åˆ—è¡¨ï¼ˆå­—èŠ‚æ•°ï¼‰
- `perfProfile`ï¼šæ€§èƒ½æ¨¡å¼å­—ç¬¦ä¸²
  - `"default"`ï¼šé»˜è®¤æ¨¡å¼
  - `"high_performance"`ï¼šé«˜æ€§èƒ½æ¨¡å¼
  - `"burst"`ï¼šçªå‘æ¨¡å¼
- `graphIndex`ï¼šå›¾ç´¢å¼•ï¼ˆå¤šå›¾æ¨¡å‹ï¼‰

**è¿”å›å€¼**ï¼š

- `true`ï¼šæ¨ç†æˆåŠŸ
- `false`ï¼šæ¨ç†å¤±è´¥

âš ï¸ **å†…å­˜ç®¡ç†é‡è¦æç¤º**ï¼š

- `outputBuffers` ä¸­çš„å†…å­˜ç”±å‡½æ•°è‡ªåŠ¨åˆ†é…
- **å¿…é¡»æ‰‹åŠ¨é‡Šæ”¾**ï¼šä½¿ç”¨ `free(outputBuffers[i])` é‡Šæ”¾æ¯ä¸ªè¾“å‡ºç¼“å†²åŒº

##### ModelDestroy - é”€æ¯æ¨¡å‹

```cpp
bool ModelDestroy(std::string model_name);
```

##### æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢æ–¹æ³•

```cpp
// è·å–è¾“å…¥å½¢çŠ¶
std::vector<std::vector<size_t>> getInputShapes(std::string model_name);

// è·å–è¾“å‡ºå½¢çŠ¶
std::vector<std::vector<size_t>> getOutputShapes(std::string model_name);

// è·å–è¾“å…¥æ•°æ®ç±»å‹
std::vector<std::string> getInputDataType(std::string model_name);

// è·å–è¾“å‡ºæ•°æ®ç±»å‹
std::vector<std::string> getOutputDataType(std::string model_name);

// è·å–å›¾åç§°
std::string getGraphName(std::string model_name);

// è·å–è¾“å…¥åç§°
std::vector<std::string> getInputName(std::string model_name);

// è·å–è¾“å‡ºåç§°
std::vector<std::string> getOutputName(std::string model_name);
```

### 4.2 æ—¥å¿—å’Œæ€§èƒ½å‡½æ•°

#### æ—¥å¿—å‡½æ•°

```cpp
// è®¾ç½®æ—¥å¿—çº§åˆ«
bool SetLogLevel(int32_t log_level, const std::string log_path = "None");

// æ—¥å¿—è¾“å‡ºå‡½æ•°
void QNN_ERR(const char* fmt, ...);  // é”™è¯¯æ—¥å¿—
void QNN_WAR(const char* fmt, ...);  // è­¦å‘Šæ—¥å¿—
void QNN_INF(const char* fmt, ...);  // ä¿¡æ¯æ—¥å¿—
void QNN_VEB(const char* fmt, ...);  // è¯¦ç»†æ—¥å¿—
void QNN_DBG(const char* fmt, ...);  // è°ƒè¯•æ—¥å¿—
```

**æ—¥å¿—çº§åˆ«å¸¸é‡**ï¼š

```cpp
#define QNN_LOG_LEVEL_ERROR   1
#define QNN_LOG_LEVEL_WARN    2
#define QNN_LOG_LEVEL_INFO    3
#define QNN_LOG_LEVEL_VERBOSE 4
#define QNN_LOG_LEVEL_DEBUG   5
```

#### æ€§èƒ½é…ç½®å‡½æ•°

```cpp
// è®¾ç½®å…¨å±€æ€§èƒ½æ¨¡å¼
bool SetPerfProfileGlobal(const std::string& perf_profile);

// é‡Šæ”¾å…¨å±€æ€§èƒ½æ¨¡å¼
bool RelPerfProfileGlobal();

// è®¾ç½®æ€§èƒ½åˆ†æçº§åˆ«
bool SetProfilingLevel(int32_t profiling_level);
```

**æ€§èƒ½åˆ†æçº§åˆ«**ï¼š

```cpp
#define QNN_PROFILING_LEVEL_OFF      0
#define QNN_PROFILING_LEVEL_BASIC    1
#define QNN_PROFILING_LEVEL_DETAILED 2
```

#### TimerHelper - è®¡æ—¶å·¥å…·ç±»

```cpp
class TimerHelper {
public:
    TimerHelper();                                     // æ„é€ å‡½æ•°ï¼ˆè‡ªåŠ¨å¼€å§‹è®¡æ—¶ï¼‰
    void Reset();                                      // é‡ç½®è®¡æ—¶å™¨
    void Print(std::string message);                   // æ‰“å°ç»è¿‡çš„æ—¶é—´
    void Print(std::string message, bool reset);       // æ‰“å°å¹¶å¯é€‰é‡ç½®
};
```

### 4.3 å®Œæ•´ C++ ç¤ºä¾‹

#### ç¤ºä¾‹ 1ï¼šå›¾åƒè¶…åˆ†è¾¨ç‡ï¼ˆReal-ESRGANï¼‰

åŸºäºçœŸå®ç¤ºä¾‹ä»£ç ï¼š`samples/C++/real_esrgan_x4plus/real_esrgan_x4plus.cpp`

```cpp
#include "LibAppBuilder.hpp"
#include <iostream>
#include <filesystem>
#include <opencv2/opencv.hpp>
#include <xtensor/xarray.hpp>
#include <xtensor/xadapt.hpp>

namespace fs = std::filesystem;

const std::string MODEL_NAME = "real_esrgan_x4plus";
const int IMAGE_SIZE = 512;
const int SCALE = 4;

#define RGB_IMAGE_SIZE_F32(width, height) ((width) * (height) * 3 * 4)

// ============================================
// è¾…åŠ©å‡½æ•°ï¼šè½¬æ¢ OpenCV Mat ä¸º xtensor
// ============================================
xt::xarray<float> ConvertTensor(cv::Mat &img, int scale) {
    int b = 1;
    int ch = img.channels();
    int hh = img.rows;
    int hw = img.cols;
    int out_channel = ch * (scale * scale);
    int h = hh / scale;
    int w = hw / scale;

    // è¾“å…¥ img æ˜¯ HWC æ ¼å¼
    size_t size = img.total();
    size_t channels = img.channels();
    std::vector<int> shape = {img.rows, img.cols, img.channels()};

    std::vector<int> reshape_scale = {b, h, scale, w, scale, ch};
    std::vector<int> reshape_final = {b, h, w, out_channel};

    // è½¬æ¢ä¸º xarray
    xt::xarray<float> input = xt::adapt(
        (float*)img.data, 
        size * channels, 
        xt::no_ownership(), 
        shape
    );

    input.reshape(reshape_scale);
    input = xt::transpose(input, {0, 1, 3, 5, 2, 4});
    input.reshape(reshape_final);

    return input;
}

int main() {
    // ============================================
    // 1. è®¾ç½®è·¯å¾„
    // ============================================
    fs::path execution_ws = fs::current_path();
    fs::path backend_lib_path = execution_ws / "QnnHtp.dll";
    fs::path system_lib_path = execution_ws / "QnnSystem.dll";
    fs::path model_path = execution_ws / (MODEL_NAME + ".bin");
    fs::path input_path = execution_ws / "input.jpg";
    fs::path output_path = execution_ws / "output.jpg";

    // ============================================
    // 2. åˆå§‹åŒ–æ—¥å¿—å’Œæ€§èƒ½åˆ†æ
    // ============================================
    SetLogLevel(QNN_LOG_LEVEL_WARN);
    SetProfilingLevel(QNN_PROFILING_LEVEL_BASIC);

    // ============================================
    // 3. åˆ›å»º LibAppBuilder å®ä¾‹å¹¶åˆå§‹åŒ–æ¨¡å‹
    // ============================================
    LibAppBuilder libAppBuilder;

    std::cout << "æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹..." << std::endl;
    int ret = libAppBuilder.ModelInitialize(
        MODEL_NAME,
        model_path.string(),
        backend_lib_path.string(),
        system_lib_path.string()
    );

    if (ret < 0) {
        std::cout << "æ¨¡å‹åŠ è½½å¤±è´¥" << std::endl;
        return -1;
    }
    std::cout << "æ¨¡å‹åˆå§‹åŒ–å®Œæˆ" << std::endl;

    // ============================================
    // 4. è¯»å–å’Œé¢„å¤„ç†å›¾åƒ
    // ============================================
    cv::Mat orig_image = cv::imread(input_path.string(), cv::IMREAD_COLOR);
    if (orig_image.empty()) {
        QNN_ERR("æ— æ³•è¯»å–å›¾åƒ: %s", input_path.string().c_str());
        return -1;
    }

    // è½¬æ¢ä¸º RGB
    cv::Mat rgb_image;
    cv::cvtColor(orig_image, rgb_image, cv::COLOR_BGR2RGB);

    // è°ƒæ•´å¤§å°
    cv::Mat resized_image;
    cv::resize(rgb_image, resized_image, cv::Size(IMAGE_SIZE, IMAGE_SIZE));

    // å½’ä¸€åŒ–åˆ° [0, 1]
    cv::Mat input_mat;
    resized_image.convertTo(input_mat, CV_32FC3, 1.0 / 255.0);

    // è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    xt::xarray<float> input_tensor = ConvertTensor(input_mat, 1);

    // åˆ†é…è¾“å…¥ç¼“å†²åŒº
    uint32_t size = RGB_IMAGE_SIZE_F32(IMAGE_SIZE, IMAGE_SIZE);
    float* input_buffer = new float[size / 4];
    std::copy(input_tensor.begin(), input_tensor.end(), input_buffer);

    // ============================================
    // 5. æ‰§è¡Œæ¨ç†
    // ============================================
    std::vector<uint8_t*> inputBuffers;
    std::vector<uint8_t*> outputBuffers;
    std::vector<size_t> outputSize;
    std::string perfProfile = "burst";

    inputBuffers.push_back(reinterpret_cast<uint8_t*>(input_buffer));

    // è®¾ç½®å…¨å±€æ€§èƒ½æ¨¡å¼
    SetPerfProfileGlobal("burst");

    std::cout << "æ­£åœ¨æ‰§è¡Œæ¨ç†..." << std::endl;
    TimerHelper timer;

    ret = libAppBuilder.ModelInference(
        MODEL_NAME,
        inputBuffers,
        outputBuffers,
        outputSize,
        perfProfile
    );

    timer.Print("æ¨ç†æ—¶é—´: ", false);

    // é‡Šæ”¾æ€§èƒ½æ¨¡å¼
    RelPerfProfileGlobal();

    if (ret < 0) {
        std::cout << "æ¨ç†å¤±è´¥" << std::endl;
        delete[] input_buffer;
        return -1;
    }
    std::cout << "æ¨ç†å®Œæˆ" << std::endl;

    // ============================================
    // 6. åå¤„ç†
    // ============================================
    float* output_data = reinterpret_cast<float*>(outputBuffers[0]);

    int output_width = IMAGE_SIZE * SCALE;
    int output_height = IMAGE_SIZE * SCALE;
    int output_channels = 3;
    int output_tensor_size = output_width * output_height * output_channels;

    // åå½’ä¸€åŒ–å¹¶è½¬æ¢ä¸º uint8
    char* buffer = new char[output_tensor_size];
    for (int i = 0; i < output_tensor_size; i++) {
        float val = output_data[i];
        buffer[i] = std::max(0.0, std::min(255.0, val * 255.0));
    }

    // åˆ›å»ºè¾“å‡ºå›¾åƒ
    cv::Mat output_mat(output_height, output_width, CV_8UC3, buffer);
    cv::Mat output_mat_bgr;
    cv::cvtColor(output_mat, output_mat_bgr, cv::COLOR_RGB2BGR);

    // ä¿å­˜å›¾åƒ
    cv::imwrite(output_path.string(), output_mat_bgr);
    std::cout << "è¾“å‡ºå›¾åƒå·²ä¿å­˜åˆ°: " << output_path.string() << std::endl;

    // æ˜¾ç¤ºå›¾åƒï¼ˆå¯é€‰ï¼‰
    cv::imshow("Output Image", output_mat_bgr);
    cv::waitKey(0);

    // ============================================
    // 7. æ¸…ç†èµ„æº
    // ============================================
    delete[] input_buffer;
    delete[] buffer;

    // é‡Šæ”¾è¾“å‡ºç¼“å†²åŒºï¼ˆé‡è¦ï¼ï¼‰
    for (size_t j = 0; j < outputBuffers.size(); j++) {
        free(outputBuffers[j]);
    }
    outputBuffers.clear();
    outputSize.clear();

    // é”€æ¯æ¨¡å‹
    libAppBuilder.ModelDestroy(MODEL_NAME);

    return 0;
}
```

#### ç¤ºä¾‹ 2ï¼šå›¾åƒåˆ†ç±»ï¼ˆBEiTï¼‰

åŸºäºçœŸå®ç¤ºä¾‹ä»£ç ï¼š`samples/C++/beit/beit.cpp`

```cpp
#include "LibAppBuilder.hpp"
#include <iostream>
#include <vector>
#include <fstream>
#include <filesystem>
#include <opencv2/opencv.hpp>
#include <xtensor/xarray.hpp>
#include <xtensor/xadapt.hpp>
#include <xtensor/xmath.hpp>

namespace fs = std::filesystem;

const int IMAGENET_DIM = 224;

// ============================================
// Softmax å‡½æ•°
// ============================================
xt::xarray<float> softmax(const xt::xarray<float>& x, std::size_t dim) {
    xt::xarray<float> exp_x = xt::exp(x);
    xt::xarray<float> sum_exp = xt::sum(exp_x, {dim}, xt::keep_dims);
    return exp_x / sum_exp;
}

// ============================================
// è½¬æ¢ OpenCV Mat ä¸º xtensorï¼ˆNCHW æ ¼å¼ï¼‰
// ============================================
xt::xarray<float> ConvertTensor(cv::Mat &img, int scale) {
    int b = 1;
    int ch = img.channels();
    int hh = img.rows;
    int hw = img.cols;
    int out_channel = ch * (scale * scale);
    int h = hh / scale;
    int w = hw / scale;

    // è¾“å…¥ img æ˜¯ HWC æ ¼å¼
    size_t size = img.total();
    size_t channels = img.channels();
    std::vector<int> shape = {img.cols, img.rows, img.channels()};

    std::vector<int> reshape_scale = {b, h, scale, w, scale, ch};
    std::vector<int> reshape_final = {b, out_channel, h, w};

    // è½¬æ¢ä¸º xarray
    xt::xarray<float> input = xt::adapt(
        (float*)img.data,
        size * channels,
        xt::no_ownership(),
        shape
    );

    input.reshape(reshape_scale);
    input = xt::transpose(input, {0, 1, 3, 5, 2, 4});
    input.reshape(reshape_final);

    return input;
}

// ============================================
// é¢„å¤„ç†å›¾åƒï¼ˆImageNet æ ‡å‡†ï¼‰
// ============================================
xt::xarray<float> preprocess_image(const cv::Mat& image) {
    cv::Mat rgb_image;
    int scale = 1;

    // è½¬æ¢ä¸º RGB
    if (image.channels() == 3) {
        cv::cvtColor(image, rgb_image, cv::COLOR_BGR2RGB);
    } else {
        cv::cvtColor(image, rgb_image, cv::COLOR_GRAY2RGB);
    }

    // 1. è°ƒæ•´å¤§å°åˆ° 256x256
    cv::Mat resized_image;
    cv::resize(rgb_image, resized_image, cv::Size(256, 256), 0, 0, cv::INTER_LINEAR);

    // 2. ä¸­å¿ƒè£å‰ªåˆ° 224x224
    int crop_x = (256 - IMAGENET_DIM) / 2;
    int crop_y = (256 - IMAGENET_DIM) / 2;
    cv::Rect roi(crop_x, crop_y, IMAGENET_DIM, IMAGENET_DIM);
    cv::Mat cropped_image = resized_image(roi).clone();

    // 3. å½’ä¸€åŒ–åˆ° [0, 1]
    cv::Mat input_mat;
    cropped_image.convertTo(input_mat, CV_32FC3, 1.0 / 255.0);

    // 4. è½¬æ¢ä¸º NCHW æ ¼å¼
    xt::xarray<float> input_tensor = ConvertTensor(input_mat, scale);

    return input_tensor;
}

// ============================================
// BEiT åˆ†ç±»å™¨ç±»
// ============================================
class BEIT {
public:
    std::string model_name = "beit";
    std::string model_path;
    std::string backend_lib;
    std::string system_lib;
    LibAppBuilder libAppBuilder;

    BEIT(const std::string& model_path,
         const std::string& backend_lib,
         const std::string& system_lib)
        : model_path(model_path),
          backend_lib(backend_lib),
          system_lib(system_lib) {}

    int LoadModel() {
        std::cout << "æ­£åœ¨åŠ è½½æ¨¡å‹..." << std::endl;
        int ret = libAppBuilder.ModelInitialize(
            model_name,
            model_path,
            backend_lib,
            system_lib,
            false  // async
        );
        std::cout << "æ¨¡å‹åŠ è½½å®Œæˆ" << std::endl;
        return ret;
    }

    int DestroyModel() {
        std::cout << "æ­£åœ¨é”€æ¯æ¨¡å‹..." << std::endl;
        int ret = libAppBuilder.ModelDestroy(model_name);
        return ret;
    }

    xt::xarray<float> predict(const cv::Mat& image) {
        std::cout << "æ­£åœ¨é¢„æµ‹..." << std::endl;

        int size = 3 * IMAGENET_DIM * IMAGENET_DIM;
        std::unique_ptr<float[]> input_buffer(new float[size]);

        // é¢„å¤„ç†å›¾åƒ
        xt::xarray<float> input_tensor = preprocess_image(image);
        std::copy(input_tensor.begin(), input_tensor.end(), input_buffer.get());

        // å‡†å¤‡è¾“å…¥è¾“å‡ºç¼“å†²åŒº
        std::vector<uint8_t*> inputBuffers;
        std::vector<uint8_t*> outputBuffers;
        std::vector<size_t> outputSize;
        std::string perfProfile = "burst";
        int graphIndex = 0;

        inputBuffers.push_back(reinterpret_cast<uint8_t*>(input_buffer.get()));

        // æ‰§è¡Œæ¨ç†
        std::cout << "æ­£åœ¨æ‰§è¡Œæ¨ç†..." << std::endl;
        int ret = libAppBuilder.ModelInference(
            model_name,
            inputBuffers,
            outputBuffers,
            outputSize,
            perfProfile,
            graphIndex
        );
        std::cout << "æ¨ç†å®Œæˆ" << std::endl;

        if (ret < 0) {
            std::cout << "æ¨ç†å¤±è´¥" << std::endl;
            return xt::zeros<float>({1000});
        }

        // å¤„ç†è¾“å‡º
        float* pred_output = reinterpret_cast<float*>(outputBuffers[0]);
        size_t output_elements = 1000;  // ImageNet ç±»åˆ«æ•°

        xt::xarray<float> output = xt::zeros<float>({output_elements});
        std::copy(pred_output, pred_output + output_elements, output.begin());

        // é‡Šæ”¾è¾“å‡ºç¼“å†²åŒº
        for (auto buffer : outputBuffers) {
            free(buffer);
        }

        // åº”ç”¨ softmax
        return softmax(output, 0);
    }
};

// ============================================
// åŠ è½½ ImageNet æ ‡ç­¾
// ============================================
std::vector<std::string> load_labels(const std::string& file_path) {
    std::vector<std::string> labels;
    std::ifstream file(file_path);

    if (file.is_open()) {
        std::string line;
        while (std::getline(file, line)) {
            labels.push_back(line);
        }
        file.close();
    }

    return labels;
}

// ============================================
// ä¸»å‡½æ•°
// ============================================
int main() {
    // è®¾ç½®è·¯å¾„
    std::string image_path = "../input.jpg";
    std::string json_path = "../models/imagenet_labels.json";
    std::string model_path = "../models/beit.bin";
    std::string backend_lib = "../qai_libs/QnnHtp.dll";
    std::string system_lib = "../qai_libs/QnnSystem.dll";

    // è¯»å–å›¾åƒ
    cv::Mat image = cv::imread(image_path);
    if (image.empty()) {
        std::cout << "æ— æ³•è¯»å–å›¾åƒ" << std::endl;
        return -1;
    }

    // åˆ›å»ºåˆ†ç±»å™¨
    BEIT beit(model_path, backend_lib, system_lib);

    // è®¾ç½®æ—¥å¿—çº§åˆ«
    SetLogLevel(QNN_LOG_LEVEL_WARN);

    // åŠ è½½æ¨¡å‹
    int ret = beit.LoadModel();
    if (ret < 0) {
        std::cout << "æ¨¡å‹åŠ è½½å¤±è´¥" << std::endl;
        return -1;
    }

    // æ‰§è¡Œé¢„æµ‹
    xt::xarray<float> probabilities = beit.predict(image);
    std::cout << "é¢„æµ‹å®Œæˆï¼Œæ¦‚ç‡æ•°ç»„å¤§å°: " << probabilities.size() << std::endl;

    // æ‰¾åˆ° Top-5 é¢„æµ‹
    std::vector<std::pair<float, int>> indexed_probs;
    for (size_t i = 0; i < probabilities.size(); ++i) {
        indexed_probs.emplace_back(probabilities[i], static_cast<int>(i));
    }
    std::sort(indexed_probs.begin(), indexed_probs.end(),
              std::greater<std::pair<float, int>>());

    // åŠ è½½æ ‡ç­¾
    std::vector<std::string> labels = load_labels(json_path);

    // æ‰“å° Top-5 ç»“æœ
    std::cout << "\nTop 5 é¢„æµ‹ç»“æœ:" << std::endl;
    for (int i = 0; i < 5; ++i) {
        int class_idx = indexed_probs[i].second;
        float prob = indexed_probs[i].first;
        std::string label = (class_idx < labels.size()) ? labels[class_idx] : "Unknown";
        std::cout << (i + 1) << ". " << label << ": " 
                  << (100 * prob) << "%" << std::endl;
    }

    // é”€æ¯æ¨¡å‹
    ret = beit.DestroyModel();
    if (ret < 0) {
        std::cout << "æ¨¡å‹é”€æ¯å¤±è´¥" << std::endl;
        return -1;
    }

    return 0;
}
```

## 5. é«˜çº§åŠŸèƒ½

### 5.1 LoRA é€‚é…å™¨æ”¯æŒ

LoRA (Low-Rank Adaptation) å…è®¸åŠ¨æ€åŠ è½½å’Œåˆ‡æ¢æ¨¡å‹é€‚é…å™¨ã€‚

#### Python ç¤ºä¾‹

```python
from qai_appbuilder import QNNLoraContext, LoraAdapter, QNNConfig, Runtime, LogLevel

# é…ç½®ç¯å¢ƒ
QNNConfig.Config(
    qnn_lib_path="./qai_libs",
    runtime=Runtime.HTP,
    log_level=LogLevel.INFO
)

# ============================================
# åˆ›å»º LoRA é€‚é…å™¨
# ============================================
adapter1 = LoraAdapter(
    graph_name="llm_graph",
    lora_file_paths=[
        "lora/adapter1_layer1.bin",
        "lora/adapter1_layer2.bin"
    ]
)

adapter2 = LoraAdapter(
    graph_name="llm_graph",
    lora_file_paths=[
        "lora/adapter2_layer1.bin",
        "lora/adapter2_layer2.bin"
    ]
)

# ============================================
# åˆå§‹åŒ–å¸¦ LoRA çš„æ¨¡å‹
# ============================================
model = QNNLoraContext(
    model_name="llm_with_lora",
    model_path="models/base_llm.bin",
    lora_adapters=[adapter1]  # åˆå§‹åŠ è½½ adapter1
)

# æ‰§è¡Œæ¨ç†
output1 = model.Inference([input_data])
print(f"ä½¿ç”¨ adapter1 çš„è¾“å‡º: {output1[0].shape}")

# ============================================
# åŠ¨æ€åˆ‡æ¢ LoRA é€‚é…å™¨
# ============================================
model.apply_binary_update([adapter2])

output2 = model.Inference([input_data])
print(f"ä½¿ç”¨ adapter2 çš„è¾“å‡º: {output2[0].shape}")

# æ¸…ç†
del model
```

### 5.2 å¤šå›¾æ¨¡å‹æ”¯æŒ

æŸäº›æ¨¡å‹åŒ…å«å¤šä¸ªè®¡ç®—å›¾ï¼Œå¯ä»¥é€šè¿‡ `graphIndex` å‚æ•°é€‰æ‹©ã€‚

```python
from qai_appbuilder import QNNContext

model = QNNContext(
    model_name="multi_graph_model",
    model_path="models/multi_graph_model.bin"
)

# ä½¿ç”¨ç¬¬ä¸€ä¸ªå›¾
output1 = model.Inference([input_data], graphIndex=0)

# ä½¿ç”¨ç¬¬äºŒä¸ªå›¾
output2 = model.Inference([input_data], graphIndex=1)

del model
```

### 5.3 æ”¯æŒçš„æ¨¡å‹æ ¼å¼

QAI AppBuilder æ”¯æŒå¤šç§æ¨¡å‹æ ¼å¼ï¼Œé€‚ç”¨äºä¸åŒçš„è¿è¡Œæ—¶å’Œåœºæ™¯ã€‚

#### 5.3.1 æ”¯æŒçš„æ¨¡å‹æ ¼å¼åˆ—è¡¨

| æ ¼å¼     | è¿è¡Œæ—¶     | è¯´æ˜                      |
| ------ | ------- | ----------------------- |
| `.bin` | HTP/CPU | é¢„ç¼–è¯‘çš„äºŒè¿›åˆ¶æ ¼å¼ï¼ŒåŠ è½½é€Ÿåº¦æœ€å¿«ï¼ˆæ¨èï¼‰    |
| `.dlc` | HTP/CPU | ä» QAIRT 2.41.0 å¼€å§‹æ”¯æŒç›´æ¥åŠ è½½ |
| `.so`  | CPU     | å…±äº«åº“æ ¼å¼ï¼Œä»…åœ¨ CPU ä¸Šè¿è¡Œ        |

#### 5.3.2 DLC æ¨¡å‹ç›´æ¥åŠ è½½

ä» QAIRT 2.41.0 ç‰ˆæœ¬å¼€å§‹ï¼Œæ”¯æŒç›´æ¥åŠ è½½ `.dlc` æ¨¡å‹æ–‡ä»¶ã€‚

```python
from qai_appbuilder import QNNContext, QNNConfig, Runtime, LogLevel

QNNConfig.Config(
    qnn_lib_path="./qai_libs",
    runtime=Runtime.HTP,
    log_level=LogLevel.INFO
)

# ============================================
# ç›´æ¥åŠ è½½ .dlc æ–‡ä»¶
# ============================================
model = QNNContext(
    model_name="my_model",
    model_path="models/my_model.dlc"  # ä½¿ç”¨ .dlc æ–‡ä»¶
)

# é¦–æ¬¡è¿è¡Œæ—¶ï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆ my_model.dlc.bin æ–‡ä»¶
# åç»­è¿è¡Œä¼šç›´æ¥åŠ è½½ .dlc.bin ä»¥æé«˜é€Ÿåº¦

output = model.Inference([input_data])

del model
```

**æ³¨æ„äº‹é¡¹**ï¼š

- é¦–æ¬¡åŠ è½½ `.dlc` æ–‡ä»¶æ—¶ä¼šè‡ªåŠ¨è½¬æ¢ä¸º `.dlc.bin`
- è½¬æ¢åçš„ `.dlc.bin` æ–‡ä»¶ä¼šä¿å­˜åœ¨åŒä¸€ç›®å½•
- åç»­è¿è¡Œä¼šç›´æ¥åŠ è½½ `.dlc.bin` æ–‡ä»¶ï¼Œé€Ÿåº¦æ›´å¿«
- å¦‚æœéœ€è¦é‡æ–°è½¬æ¢ï¼Œåˆ é™¤ `.dlc.bin` æ–‡ä»¶å³å¯

#### 5.3.3 SO æ¨¡å‹æ ¼å¼ï¼ˆCPU è¿è¡Œæ—¶ï¼‰

å¯¹äºéœ€è¦åœ¨ CPU ä¸Šè¿è¡Œçš„æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ `.so` æ ¼å¼ï¼š

```python
from qai_appbuilder import QNNContext, QNNConfig, Runtime, LogLevel

# é…ç½®ä¸º CPU è¿è¡Œæ—¶
QNNConfig.Config(
    qnn_lib_path="./qai_libs",
    runtime=Runtime.CPU,  # ä½¿ç”¨ CPU è¿è¡Œæ—¶
    log_level=LogLevel.INFO
)

# åŠ è½½ .so æ¨¡å‹æ–‡ä»¶
model = QNNContext(
    model_name="cpu_model",
    model_path="models/my_model.so"  # ä½¿ç”¨ .so æ–‡ä»¶
)

output = model.Inference([input_data])

del model
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 ä½¿ç”¨ Native æ¨¡å¼ï¼ˆæ¨èï¼‰

**æ€§èƒ½æå‡**ï¼šNative æ¨¡å¼å¯ä»¥å‡å°‘ 10%-200% çš„æ•°æ®è½¬æ¢å¼€é”€ã€‚

```python
# âŒ Float æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰- æœ‰è½¬æ¢å¼€é”€
model_float = QNNContext(
    model_name="model",
    model_path="model.bin",
    input_data_type=DataType.FLOAT,
    output_data_type=DataType.FLOAT
)

# âœ… Native æ¨¡å¼ - æ— è½¬æ¢å¼€é”€ï¼Œæ€§èƒ½æ›´å¥½
model_native = QNNContext(
    model_name="model",
    model_path="model.bin",
    input_data_type=DataType.NATIVE,
    output_data_type=DataType.NATIVE
)
```

### 6.2 ä½¿ç”¨ Burst æ€§èƒ½æ¨¡å¼

åœ¨éœ€è¦æœ€é«˜æ€§èƒ½çš„åœºæ™¯ä¸‹ä½¿ç”¨ Burst æ¨¡å¼ã€‚

```python
from qai_appbuilder import PerfProfile

# âœ… å…¨å±€è®¾ç½®ï¼ˆç”¨äºæ‰¹é‡æ¨ç†ï¼‰
PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)

for i in range(100):
    output = model.Inference([input_data], perf_profile=PerfProfile.DEFAULT)

PerfProfile.RelPerfProfileGlobal()
```

### 6.3 æ‰¹é‡æ¨ç†ä¼˜åŒ–

```python
# âŒ ä¸æ¨èï¼šæ¯æ¬¡éƒ½åˆå§‹åŒ–æ¨¡å‹
for data in dataset:
    model = QNNContext(...)
    output = model.Inference([data])
    del model

# âœ… æ¨èï¼šåªåˆå§‹åŒ–ä¸€æ¬¡
model = QNNContext(...)

PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)
for data in dataset:
    output = model.Inference([data])
PerfProfile.RelPerfProfileGlobal()

del model
```

### 6.4 ä½¿ç”¨ ARM64 Pythonï¼ˆWindowsï¼‰

åœ¨ Windows on Snapdragon å¹³å°ä¸Šï¼ŒARM64 Python æ¯” x64 Python æ€§èƒ½æ›´å¥½ã€‚

```bash
# ä¸‹è½½ ARM64 Python
https://www.python.org/ftp/python/3.12.8/python-3.12.8-arm64.exe

# å®‰è£… ARM64 ç‰ˆæœ¬çš„ QAI AppBuilder
pip install qai_appbuilder-{version}-cp312-cp312-win_arm64.whl
```

## 7. å¸¸è§é—®é¢˜

### 7.1 æ¨¡å‹åŠ è½½å¤±è´¥

**é—®é¢˜**ï¼š`ModelInitialize` è¿”å› `False` æˆ–å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
from qai_appbuilder import QNNConfig, LogLevel, Runtime
from pathlib import Path

# 1. å¯ç”¨è¯¦ç»†æ—¥å¿—
QNNConfig.Config(
    runtime=Runtime.HTP,
    log_level=LogLevel.DEBUG,  # ä½¿ç”¨ DEBUG çº§åˆ«
    log_path="./debug.log"     # è¾“å‡ºåˆ°æ–‡ä»¶
)

# 2. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
model_path = Path("models/my_model.bin")
if not model_path.exists():
    print(f"é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    exit()
```

### 7.2 æ¨ç†ç»“æœä¸æ­£ç¡®

**é—®é¢˜**ï¼šæ¨ç†è¾“å‡ºä¸é¢„æœŸä¸ç¬¦

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# 1. éªŒè¯æ¨¡å‹ä¿¡æ¯
print(f"è¾“å…¥å½¢çŠ¶: {model.getInputShapes()}")
print(f"è¾“å‡ºå½¢çŠ¶: {model.getOutputShapes()}")
print(f"è¾“å…¥æ•°æ®ç±»å‹: {model.getInputDataType()}")
print(f"è¾“å‡ºæ•°æ®ç±»å‹: {model.getOutputDataType()}")

# 2. éªŒè¯è¾“å…¥æ•°æ®
expected_shape = model.getInputShapes()[0]
print(f"æœŸæœ›å½¢çŠ¶: {expected_shape}")
print(f"å®é™…å½¢çŠ¶: {input_data.shape}")
print(f"å®é™…æ•°æ®ç±»å‹: {input_data.dtype}")

# 3. æ£€æŸ¥æ•°æ®èŒƒå›´
print(f"è¾“å…¥æ•°æ®èŒƒå›´: [{input_data.min()}, {input_data.max()}]")

# 4. æ£€æŸ¥è¾“å‡º
output = model.Inference([input_data])
print(f"è¾“å‡ºå½¢çŠ¶: {output[0].shape}")
print(f"è¾“å‡ºæ•°æ®ç±»å‹: {output[0].dtype}")
print(f"è¾“å‡ºæ•°æ®èŒƒå›´: [{output[0].min()}, {output[0].max()}]")
```

### 7.3 å†…å­˜æ³„æ¼

**Python è§£å†³æ–¹æ¡ˆ**ï¼š

```python
import gc

# æ˜¾å¼åˆ é™¤æ¨¡å‹
model = QNNContext(...)
# ... ä½¿ç”¨æ¨¡å‹ ...
del model
gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
```

**C++ è§£å†³æ–¹æ¡ˆ**ï¼š

```cpp
// å¿…é¡»æ‰‹åŠ¨é‡Šæ”¾è¾“å‡ºç¼“å†²åŒº
for (auto buffer : outputBuffers) {
    free(buffer);  // é‡Šæ”¾æ¯ä¸ªè¾“å‡ºç¼“å†²åŒº
}
outputBuffers.clear();
outputSize.clear();

// é”€æ¯æ¨¡å‹
appBuilder.ModelDestroy(model_name);
```

### 7.4 Native æ¨¡å¼æ•°æ®ç±»å‹ä¸åŒ¹é…

**é—®é¢˜**ï¼šNative æ¨¡å¼ä¸‹æ•°æ®ç±»å‹é”™è¯¯å¯¼è‡´æ¨ç†å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
from qai_appbuilder import QNNContext, DataType
import numpy as np

# 1. ä½¿ç”¨ Native æ¨¡å¼åˆå§‹åŒ–
model = QNNContext(
    model_name="model",
    model_path="model.bin",
    input_data_type=DataType.NATIVE,
    output_data_type=DataType.NATIVE
)

# 2. æŸ¥è¯¢æ¨¡å‹è¦æ±‚çš„æ•°æ®ç±»å‹
input_dtypes = model.getInputDataType()
input_shapes = model.getInputShapes()

print(f"æ¨¡å‹è¦æ±‚çš„è¾“å…¥æ•°æ®ç±»å‹: {input_dtypes}")
print(f"æ¨¡å‹è¦æ±‚çš„è¾“å…¥å½¢çŠ¶: {input_shapes}")

# 3. åˆ›å»ºæ•°æ®ç±»å‹æ˜ å°„
dtype_map = {
    'float16': np.float16,
    'fp16': np.float16,
    'float32': np.float32,
    'fp32': np.float32,
    'float': np.float32,
    'int8': np.int8,
    'uint8': np.uint8,
    'int16': np.int16,
    'uint16': np.uint16,
    'int32': np.int32,
    'uint32': np.uint32,
    'bool': np.bool_
}

# 4. æ ¹æ®æ¨¡å‹è¦æ±‚å‡†å¤‡æ•°æ®
input_dtype_str = input_dtypes[0].lower()
input_dtype = dtype_map.get(input_dtype_str, np.float32)

print(f"ä½¿ç”¨æ•°æ®ç±»å‹: {input_dtype}")

# 5. åˆ›å»ºæ­£ç¡®ç±»å‹çš„è¾“å…¥æ•°æ®
input_data = np.random.randn(*input_shapes[0]).astype(input_dtype)

print(f"è¾“å…¥æ•°æ®ç±»å‹: {input_data.dtype}")
print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}")

# 6. æ‰§è¡Œæ¨ç†
output = model.Inference([input_data])

print(f"è¾“å‡ºæ•°æ®ç±»å‹: {output[0].dtype}")
print(f"è¾“å‡ºæ•°æ®å½¢çŠ¶: {output[0].shape}")
```

### 7.5 C++ é“¾æ¥é”™è¯¯

**é—®é¢˜**ï¼šLNK2038ã€LNK2001 æˆ–å…¶ä»–é“¾æ¥é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š

ç¡®ä¿ Visual Studio é¡¹ç›®é…ç½®æ­£ç¡®ï¼š

1. **è¿è¡Œæ—¶åº“**ï¼ˆæœ€å¸¸è§é—®é¢˜ï¼‰
   
   - é¡¹ç›®å±æ€§ â†’ C/C++ â†’ ä»£ç ç”Ÿæˆ â†’ è¿è¡Œæ—¶åº“
   - å¿…é¡»è®¾ç½®ä¸ºï¼š**å¤šçº¿ç¨‹ DLL (/MD)**

2. **å¹³å°**
   
   - é¡¹ç›®å±æ€§ â†’ å¸¸è§„ â†’ å¹³å°
   - è®¾ç½®ä¸ºï¼š**ARM64**ï¼ˆå¯¹äº WoS å¹³å°ï¼‰

3. **é…ç½®**
   
   - ä½¿ç”¨ **Release** é…ç½®ï¼ˆè€Œé Debugï¼‰

4. **C++ æ ‡å‡†**
   
   - é¡¹ç›®å±æ€§ â†’ C/C++ â†’ è¯­è¨€ â†’ C++ è¯­è¨€æ ‡å‡†
   - è®¾ç½®ä¸ºï¼š**ISO C++17** æˆ–æ›´é«˜

5. **å­—ç¬¦é›†**
   
   - é¡¹ç›®å±æ€§ â†’ é«˜çº§ â†’ å­—ç¬¦é›†
   - è®¾ç½®ä¸ºï¼š**ä½¿ç”¨ Unicode å­—ç¬¦é›†**

### 7.6 æ€§èƒ½ä¸ä½³

**é—®é¢˜**ï¼šæ¨ç†é€Ÿåº¦æ…¢äºé¢„æœŸ

**è¯Šæ–­å’Œè§£å†³**ï¼š

```python
from qai_appbuilder import QNNConfig, Runtime, LogLevel, ProfilingLevel, DataType, PerfProfile
import time

# 1. ç¡®ä¿ä½¿ç”¨ HTPï¼ˆNPUï¼‰è€Œé CPU
QNNConfig.Config(
    qnn_lib_path="./qai_libs",
    runtime=Runtime.HTP,  # ç¡®ä¿æ˜¯ HTP
    log_level=LogLevel.INFO,
    profiling_level=ProfilingLevel.BASIC  # å¯ç”¨æ€§èƒ½åˆ†æ
)

# 2. ä½¿ç”¨ Native æ¨¡å¼
model = QNNContext(
    model_name="model",
    model_path="model.bin",
    input_data_type=DataType.NATIVE,
    output_data_type=DataType.NATIVE
)

# 3. ä½¿ç”¨ Burst æ¨¡å¼
PerfProfile.SetPerfProfileGlobal(PerfProfile.BURST)

# 4. æµ‹è¯•æ€§èƒ½
start_time = time.time()
for _ in range(100):
    output = model.Inference([input_data])
end_time = time.time()

avg_time = (end_time - start_time) / 100
print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time * 1000:.2f} ms")

PerfProfile.RelPerfProfileGlobal()
```

---

## 8. å‚è€ƒèµ„æº

### 8.1 å®˜æ–¹æ–‡æ¡£å’Œèµ„æº

- **GitHub ä»“åº“**ï¼šhttps://github.com/quic/ai-engine-direct-helper
- **Qualcomm AI Hub**ï¼šhttps://aihub.qualcomm.com/
- **AI Dev Home**ï¼šhttps://www.aidevhome.com/
- **QualcommÂ® AI Runtime SDK**ï¼šhttps://softwarecenter.qualcomm.com/#/catalog/item/Qualcomm_AI_Runtime_SDK

### 8.2 æ•™ç¨‹å’Œåšå®¢

- [QAI_AppBuilder: è®©æœ¬åœ° AI éƒ¨ç½²è§¦æ‰‹å¯åŠï¼](https://docs.qualcomm.com/bundle/publicresource/80-94755-1_REV_AA_QAI_AppBuilder_-_WoS.pdf)
- [å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—(1): 3åˆ†é’Ÿä¸Šæ‰‹ï¼Œåœ¨éªé¾™AI PCä¸Šéƒ¨ç½²DeepSeek!](https://blog.csdn.net/csdnsqst0050/article/details/149425691)
- [å¤§è¯­è¨€æ¨¡å‹ç³»åˆ—(2): æœ¬åœ° OpenAI å…¼å®¹ API æœåŠ¡çš„é…ç½®ä¸éƒ¨ç½²](https://blog.csdn.net/csdnsqst0050/article/details/150208814)
- [é«˜é€šå¹³å°å¤§è¯­è¨€æ¨¡å‹ç²¾é€‰](https://www.aidevhome.com/?id=51)
- [QAI AppBuilder on Linux (QCS6490)](https://docs.radxa.com/en/dragon/q6a/app-dev/npu-dev/qai-appbuilder)

### 8.3 ç¤ºä¾‹ä»£ç 

- **Python ç¤ºä¾‹**ï¼šhttps://github.com/quic/ai-engine-direct-helper/tree/main/samples/python
  
  - Real-ESRGANï¼ˆå›¾åƒè¶…åˆ†è¾¨ç‡ï¼‰
  - YOLOv8ï¼ˆç›®æ ‡æ£€æµ‹ï¼‰
  - Whisperï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰
  - Stable Diffusionï¼ˆæ–‡ç”Ÿå›¾ï¼‰
  - BEiTï¼ˆå›¾åƒåˆ†ç±»ï¼‰
  - OpenPoseï¼ˆå§¿æ€ä¼°è®¡ï¼‰
  - Depth Anythingï¼ˆæ·±åº¦ä¼°è®¡ï¼‰
  - ç­‰ 20+ ä¸ªç¤ºä¾‹

- **C++ ç¤ºä¾‹**ï¼šhttps://github.com/quic/ai-engine-direct-helper/tree/main/samples/c++
  
  - Real-ESRGAN
  - BEiTï¼ˆå›¾åƒåˆ†ç±»ï¼‰

- **WebUI åº”ç”¨**ï¼šhttps://github.com/quic/ai-engine-direct-helper/tree/main/samples/webui
  
  - ImageRepairAppï¼ˆå›¾åƒä¿®å¤ï¼‰
  - StableDiffusionAppï¼ˆæ–‡ç”Ÿå›¾ï¼‰
  - GenieWebUIï¼ˆLLM å¯¹è¯ï¼‰

### 8.4 æ¨¡å‹èµ„æº

- **AI Hub æ¨¡å‹åº“**ï¼šhttps://aihub.qualcomm.com/compute/models
- **AI Dev Home æ¨¡å‹åº“**ï¼šhttps://www.aidevhome.com/data/models/
- **Qwen2 7B SSD**ï¼šhttps://www.aidevhome.com/data/adh2/models/8380/qwen2_7b_ssd_250702.html
- **DeepSeek-R1-Distill-Qwen-7B**ï¼šhttps://aiot.aidlux.com/zh/models/detail/78

---

## 9. å¿«é€Ÿå¼€å§‹æŒ‡å—

### 9.1 ç¬¬ä¸€ä¸ª Python ç¨‹åº

```python
from qai_appbuilder import QNNContext, QNNConfig, Runtime, LogLevel
import numpy as np

# 1. é…ç½®ç¯å¢ƒï¼ˆå¿…éœ€ï¼‰
QNNConfig.Config(
    qnn_lib_path="./qai_libs",
    runtime=Runtime.HTP,
    log_level=LogLevel.INFO
)

# 2. åŠ è½½æ¨¡å‹
model = QNNContext(
    model_name="my_first_model",
    model_path="models/my_model.bin"
)

# 3. å‡†å¤‡è¾“å…¥
input_shape = model.getInputShapes()[0]
input_data = np.random.randn(*input_shape).astype(np.float32)

# 4. æ‰§è¡Œæ¨ç†
output = model.Inference([input_data])

# 5. æŸ¥çœ‹ç»“æœ
print(f"è¾“å‡ºå½¢çŠ¶: {output[0].shape}")
print(f"è¾“å‡ºæ•°æ®ç±»å‹: {output[0].dtype}")

# 6. æ¸…ç†
del model
```

### 9.2 ç¬¬ä¸€ä¸ª C++ ç¨‹åº

```cpp
#include "LibAppBuilder.hpp"
#include <iostream>

int main() {
    // 1. è®¾ç½®æ—¥å¿—
    SetLogLevel(QNN_LOG_LEVEL_INFO);

    // 2. åˆ›å»º AppBuilder
    LibAppBuilder appBuilder;

    // 3. åˆå§‹åŒ–æ¨¡å‹
    bool success = appBuilder.ModelInitialize(
        "my_first_model",
        "models/my_model.bin",
        "qai_libs/QnnHtp.dll",
        "qai_libs/QnnSystem.dll"
    );

    if (!success) {
        QNN_ERR("æ¨¡å‹åˆå§‹åŒ–å¤±è´¥");
        return -1;
    }

    // 4. å‡†å¤‡è¾“å…¥
    auto inputShapes = appBuilder.getInputShapes("my_first_model");
    size_t input_size = 1;
    for (auto dim : inputShapes[0]) {
        input_size *= dim;
    }

    float* input_data = new float[input_size];
    // ... å¡«å……è¾“å…¥æ•°æ® ...

    std::vector<uint8_t*> inputBuffers;
    inputBuffers.push_back(reinterpret_cast<uint8_t*>(input_data));

    // 5. æ‰§è¡Œæ¨ç†
    std::vector<uint8_t*> outputBuffers;
    std::vector<size_t> outputSize;
    std::string perfProfile = "burst";

    success = appBuilder.ModelInference(
        "my_first_model",
        inputBuffers,
        outputBuffers,
        outputSize,
        perfProfile
    );

    if (!success) {
        QNN_ERR("æ¨ç†å¤±è´¥");
        delete[] input_data;
        return -1;
    }

    // 6. å¤„ç†è¾“å‡º
    float* output_data = reinterpret_cast<float*>(outputBuffers[0]);
    // ... å¤„ç†è¾“å‡º ...

    // 7. æ¸…ç†
    delete[] input_data;
    for (auto buffer : outputBuffers) {
        free(buffer);
    }
    appBuilder.ModelDestroy("my_first_model");

    return 0;
}
```

---

## 10. ç‰ˆæœ¬å†å²

### v2.0.0ï¼ˆ2025å¹´1æœˆ - é‡å¤§æ›´æ–°ï¼‰

**ä¸»è¦æ–°ç‰¹æ€§**ï¼š

- âœ… **ç®€åŒ–éƒ¨ç½²**ï¼šPython æ‰©å±•åŒ…å«æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åº“ï¼ˆåŒ…æ‹¬ QAIRT SDK è¿è¡Œæ—¶ï¼‰
- âœ… **å¤šæ¨¡æ€æ”¯æŒ**ï¼šå¯¹å¤šæ¨¡æ€æ¨¡å‹ (Qwen2.5-3B-VL / Qwen2.5-3B-omini) çš„æ”¯æŒã€‚
- âœ… **DLC æ”¯æŒ**ï¼šæ”¯æŒç›´æ¥åŠ è½½ `.dlc` æ¨¡å‹æ–‡ä»¶ï¼ˆQAIRT 2.41.0+ï¼‰
- âœ… **LLM ä¼˜åŒ–**ï¼šæ–°å¢ `GenieContext` ç±»ï¼Œä¸“ä¸ºå¤§è¯­è¨€æ¨¡å‹ä¼˜åŒ–
- âœ… **æ€§èƒ½æå‡**ï¼šæ”¹è¿› Native æ¨¡å¼ï¼Œå‡å°‘æ•°æ®è½¬æ¢å¼€é”€
- âœ… **å¢å¼ºåˆ†æ**ï¼šæ”¹è¿›æ€§èƒ½åˆ†æåŠŸèƒ½ï¼Œæä¾›æ›´è¯¦ç»†çš„æ€§èƒ½æ•°æ®

**API å˜æ›´**ï¼š

- `QNNConfig.Config()` çš„ `qnn_lib_path` å‚æ•°ç°åœ¨å¯é€‰ï¼ˆé»˜è®¤ä½¿ç”¨å†…ç½®åº“ï¼‰
- `QNNContext` çš„ `backend_lib_path` å’Œ `system_lib_path` å‚æ•°ç°åœ¨å¯é€‰
- æ–°å¢ `GenieContext` ç±»ç”¨äº LLM æ¨ç†

**å·²çŸ¥é—®é¢˜**ï¼š

- æŸäº› ARM64 Python æ‰©å±•å¯èƒ½éœ€è¦æ‰‹åŠ¨ç¼–è¯‘
- Linux å¹³å°ä¸ŠæŸäº›æ¨¡å‹å¯èƒ½éœ€è¦è®¾ç½® `ADSP_LIBRARY_PATH`

### v1.xï¼ˆå†å²ç‰ˆæœ¬ï¼‰

**v1.5.0**ï¼ˆ2024å¹´12æœˆï¼‰ï¼š
- æ·»åŠ  LoRA é€‚é…å™¨æ”¯æŒ
- æ”¹è¿›å¤šå›¾æ¨¡å‹æ”¯æŒ
- ä¼˜åŒ–å†…å­˜ç®¡ç†

**v1.0.0**ï¼ˆ2024å¹´10æœˆï¼‰ï¼š
- é¦–æ¬¡æ­£å¼å‘å¸ƒ
- æ”¯æŒ Python å’Œ C++ API
- æ”¯æŒ Windows å’Œ Linux å¹³å°
- æ”¯æŒ HTP å’Œ CPU è¿è¡Œæ—¶

---

## 11. è®¸å¯è¯

QAI AppBuilder é‡‡ç”¨ **BSD 3-Clause "New" or "Revised" License**ã€‚

è¯¦è§ï¼šhttps://github.com/quic/ai-engine-direct-helper/blob/main/LICENSE

---

## 12. å…è´£å£°æ˜

æœ¬è½¯ä»¶æŒ‰"åŸæ ·"æä¾›ï¼Œä¸æä¾›ä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ã€‚ä½œè€…å’Œè´¡çŒ®è€…ä¸å¯¹å› ä½¿ç”¨æœ¬è½¯ä»¶è€Œäº§ç”Ÿçš„ä»»ä½•æŸå®³æ‰¿æ‹…è´£ä»»ã€‚ä»£ç å¯èƒ½ä¸å®Œæ•´æˆ–æµ‹è¯•ä¸å……åˆ†ã€‚ç”¨æˆ·éœ€è‡ªè¡Œè¯„ä¼°å…¶é€‚ç”¨æ€§å¹¶æ‰¿æ‹…æ‰€æœ‰ç›¸å…³é£é™©ã€‚

**æ³¨æ„**ï¼šæ¬¢è¿è´¡çŒ®ã€‚åœ¨å…³é”®ç³»ç»Ÿä¸­éƒ¨ç½²å‰è¯·ç¡®ä¿å……åˆ†æµ‹è¯•ã€‚

---

## 13. è´¡çŒ®å’Œæ”¯æŒ

### æŠ¥å‘Šé—®é¢˜

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·è®¿é—®ï¼š

- **GitHub Issues**ï¼šhttps://github.com/quic/ai-engine-direct-helper/issues
- **GitHub Discussions**ï¼šhttps://github.com/quic/ai-engine-direct-helper/discussions

### è´¡çŒ®ä»£ç 

æ¬¢è¿æäº¤ Pull Requestï¼è¯·å‚é˜…ï¼š

- **è´¡çŒ®æŒ‡å—**ï¼šhttps://github.com/quic/ai-engine-direct-helper/blob/main/CONTRIBUTING.md
- **è¡Œä¸ºå‡†åˆ™**ï¼šhttps://github.com/quic/ai-engine-direct-helper/blob/main/CODE-OF-CONDUCT.md

---

<div align="center">
  <p>â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ª Starï¼</p>
  <p>ğŸ“§ æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Ÿè®¿é—® <a href="https://github.com/quic/ai-engine-direct-helper">GitHub ä»“åº“</a></p>
</div>

---

**æ–‡æ¡£ç‰ˆæœ¬**ï¼š2.1  
**æœ€åæ›´æ–°**ï¼š2025å¹´1æœˆ26æ—¥  
**é€‚ç”¨äº**ï¼šQAI AppBuilder v2.0.0+
